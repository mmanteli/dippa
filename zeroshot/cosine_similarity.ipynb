{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524f8052-fcaf-47e8-b0fb-33dd87eb35cd",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Redaction of PII in these steps:\n",
    "\n",
    "- Select $n$ most probable subtitutions\n",
    "- from these, extract $k$ most similar in terms of cosine similarity (hyper-sphere)\n",
    "- select randomly\n",
    "\n",
    "For multisubtoken words, nothing is said(?).\n",
    "=> Trying by finding the most similar to the entire word. E.g.\n",
    "\n",
    "- original = contex1 + \"Amanda\" + context2\n",
    "- tokenized = \"Aman\" \"da\"\n",
    "- redaction result = yes\n",
    "- subtitution by token:\n",
    "1. for context1 + [MASK] [MASK] + context2 the most probable predictions for the **first masked** are $S_1$\n",
    "2. Find $S^r_1$ closest to \"Amanda\" where \"Amanda\" is max pooled form \"Aman\" and \"da\"\n",
    "3. select $s_1$ in $S^r_1$ randomly\n",
    "4. $S_2$ == most probable for context + s_1 + [MASK] + context2\n",
    "5. Similarly find one closest to \"Amanda\" => s_2\n",
    "6. Assume $s_1$+$s_2$ is a coherent word :)\n",
    "\n",
    "Notes:\n",
    "\n",
    "- if using the same model to redact and find substitutions: less computation\n",
    "- if using different models, you may optimize both\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6fdbf2-6cbd-44a8-a165-8ec42716da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, pipelines, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666182e9-ea98-49e2-9151-4a300d634051",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"xlm-roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ad8a3c-e5e0-456b-9913-864f64097331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking these from past course work:\n",
    "\n",
    "def get_embed_for_full_dataset(dataset, lang):\n",
    "  model_name = MODEL_NAME\n",
    "  p=pipeline(task=\"feature-extraction\",model=model_name,return_tensors=True,device=0)\n",
    "  embedded=p(pipelines.pt_utils.KeyDataset(dataset[lang], \"text\"), batch_size=64, truncation=\"only_first\")\n",
    "\n",
    "  # to cpu and take the mean over words\n",
    "  embedded_pooled=[torch.mean(elem,axis=1).cpu() for elem in embedded]\n",
    "  # to single matrix\n",
    "  results=torch.vstack(embedded_pooled).numpy()\n",
    "  return results\n",
    "\n",
    "def get_embed_for_one_instance(x):\n",
    "    p=pipeline(task=\"feature-extraction\",model=MODEL_NAME,return_tensors=True,device=0)\n",
    "    return p(x)\n",
    "\n",
    "def cosine_sim(x,y):\n",
    "  M=cosine_similarity(x,y)\n",
    "  aligned=np.argsort(-M,axis=-1)\n",
    "\n",
    "  sims=[]\n",
    "  for i in range(M.shape[0]): #M.shape[0] is the number of rows / input documents\n",
    "    j=aligned[i,0] # index 1 for 2nd best match => first one if different languages\n",
    "    score=M[i,j]\n",
    "    sims.append((i,j,score))\n",
    "  # sort in descending order  element -> score => sort by score\n",
    "  sims.sort(key=lambda element:element[2],reverse=True)\n",
    "\n",
    "  return sims\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b829b348-4396-4c76-9055-b9266d2f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is Leena and I like playing the piano\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9586dc21-0dbd-401b-b35c-44a3d6326105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2646,  9351,    83, 19824,    76,   136,    87,  1884, 75169,\n",
      "            70, 16569,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['<s>', '▁My', '▁name', '▁is', '▁Lee', 'na', '▁and', '▁I', '▁like', '▁playing', '▁the', '▁piano', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tok = tokenizer(text, return_tensors='pt')\n",
    "print(tok)\n",
    "detok = tokenizer.convert_ids_to_tokens(tok[\"input_ids\"][0])\n",
    "print(detok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "111b2c62-dbe2-423d-aa0d-9dd8fe1f1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee\n"
     ]
    }
   ],
   "source": [
    "# to redact: indices 4,5.\n",
    "redact_ids = np.array([4,5])\n",
    "to_redict = tok.input_ids[0][**redact_ids]\n",
    "to_substitute = tokenizer.decode(to_redact)\n",
    "good_predictions_1 = [\"Mai\", \"An\", \"Emi\", \"Luci\"]\n",
    "choose_index = 1 # An\n",
    "print(to_substitute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81785e0e-26a9-4607-93a9-5eead28b7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embed\n",
    "orig = get_embed_for_one_instance(to_substitute)\n",
    "guesses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56b3c9b-f578-4a07-ae98-c2d0b52d84e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0807,  0.1047,  0.0497,  ..., -0.1252,  0.0364,  0.0175],\n",
      "         [-0.0448, -0.0033, -0.0233,  ..., -0.0942, -0.0756, -0.0872],\n",
      "         [ 0.0683,  0.0952, -0.0115,  ..., -0.2160, -0.0362,  0.0587]]])\n"
     ]
    }
   ],
   "source": [
    "print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768e65b-d8ba-4469-b6c3-d16fc8f3cb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
