{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524f8052-fcaf-47e8-b0fb-33dd87eb35cd",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Redaction of PII in these steps:\n",
    "\n",
    "- Select $n$ most probable subtitutions\n",
    "- from these, extract $k$ most similar in terms of cosine similarity (hyper-sphere)\n",
    "- select randomly\n",
    "\n",
    "For multisubtoken words, nothing is said(?).\n",
    "=> Trying by finding the most similar to the entire word. E.g.\n",
    "\n",
    "- original = contex1 + \"Amanda\" + context2\n",
    "- tokenized = \"Aman\" \"da\"\n",
    "- redaction result = yes\n",
    "- subtitution by token:\n",
    "1. for context1 + [MASK] [MASK] + context2 the most probable predictions for the **first masked** are $S_1$\n",
    "2. Find $S^r_1$ closest to \"Amanda\" where \"Amanda\" is max pooled form \"Aman\" and \"da\"\n",
    "3. select $s_1$ in $S^r_1$ randomly\n",
    "4. $S_2$ == most probable for context + s_1 + [MASK] + context2\n",
    "5. Similarly find one closest to \"Amanda\" => s_2\n",
    "6. Assume $s_1$+$s_2$ is a coherent word :)\n",
    "\n",
    "Notes:\n",
    "\n",
    "- if using the same model to redact and find substitutions: less computation\n",
    "- if using different models, you may optimize both\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb6fdbf2-6cbd-44a8-a165-8ec42716da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, pipelines, AutoTokenizer, AutoModelForPreTraining\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58268bbd-d40e-4c23-ba9a-78b0b980d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from piimasker import PiiMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666182e9-ea98-49e2-9151-4a300d634051",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"TurkuNLP/bert-base-finnish-cased-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28ad8a3c-e5e0-456b-9913-864f64097331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking these from past course work:\n",
    "\n",
    "def get_embed_for_full_dataset(dataset, lang, pipeline):\n",
    "  model_name = MODEL_NAME\n",
    "  #p=pipeline(task=\"feature-extraction\",model=model_name,return_tensors=True,device=0)\n",
    "  embedded=pipeline(pipelines.pt_utils.KeyDataset(dataset[lang], \"text\"), batch_size=64, truncation=\"only_first\")\n",
    "\n",
    "  # to cpu and take the mean over words\n",
    "  embedded_pooled=[torch.mean(elem,axis=1).cpu() for elem in embedded]\n",
    "  # to single matrix\n",
    "  results=torch.vstack(embedded_pooled).numpy()\n",
    "  return results\n",
    "\n",
    "def get_embed_for_one_instance(x, pipeline):\n",
    "    #p=pipeline(task=\"feature-extraction\",model=MODEL_NAME,return_tensors=True,device=0)\n",
    "    embedded = pipeline(x)\n",
    "    embedded_pooled=[torch.mean(elem,axis=1).cpu() for elem in embedded]\n",
    "    results=torch.vstack(embedded_pooled).numpy()\n",
    "    return results\n",
    "\n",
    "def cosine_sim(x,y):\n",
    "  M=cosine_similarity(x,y)\n",
    "  aligned=np.argsort(-M,axis=-1)\n",
    "\n",
    "  sims=[]\n",
    "  for i in range(M.shape[0]): #M.shape[0] is the number of rows / input documents\n",
    "    j=aligned[i,1] # index 1 for 2nd best match => index [0] gives the same words.\n",
    "    score=M[i,j]\n",
    "    sims.append((i,j,score))\n",
    "  # sort in descending order  element -> score => sort by score\n",
    "  sims.sort(key=lambda element:element[2],reverse=True)\n",
    "\n",
    "  return sims\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b829b348-4396-4c76-9055-b9266d2f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Minun nimeni on Marjukka ja tykkään soittaa pianoa\"\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "pipe = pipeline(task=\"feature-extraction\",model=MODEL_NAME,return_tensors=True,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9586dc21-0dbd-401b-b35c-44a3d6326105",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PiiMasker(model, tokenizer, 1e-3, tokenizer_type=\"WordPiece\")\n",
    "output = pf.find_pii(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32cd730e-c309-469d-adfe-ddd840ada6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoded_text': '[CLS] Minun nimeni on Marjukka ja tykkään soittaa pianoa [SEP]', 'tokenizer_output': {'input_ids': tensor([[  102,  5243, 38160,   145,  1030,  1927,   357,   142,  9966,  7081,\n",
      "         37837, 50006,   103]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'to_redact_indices': [[4, 5, 6]], 'to_redact_words': [['Mar', '##ju', '##kka']], 'predictions': [[['Ri', 'Ra', 'E', 'El', 'Vil', 'Ti', 'Jo', 'I', 'An', 'J', 'Ki', 'Kari', 'Jani', 'La', 'Ro', 'Aa', 'Ne', 'Pe', 'Un', 'Jan', 'Mari', 'Te', 'Mai', 'Mi', 'O', 'Ai', 'Se', 'Eli', 'Meri', 'Erik', 'As', 'Juha', 'Aar', 'Isa', 'Vi', 'Kä', 'Li', 'Ir', 'Al', 'Lil', 'Antti', 'Mati', 'Ni', 'Y', 'Per', 'Her', 'Satu', 'S', 'Pa', 'C', 'Matti', 'Ly', 'U', 'Var', 'Jukka', 'Jorma', 'Ei', 'Za', 'Tor', 'F', 'Ari', 'En', 'Lu', 'G', 'Hei', 'Elli', 'Lauri', 'Rei', 'Es', 'Er', 'W', 'Ha', 'Kas', 'Ta', 'Pekka', 'M', 'Na', 'Pir', 'Anna', 'Val', 'Ts', 'Tiina', 'Janne', 'Martti', 'Mar', 'Timo', 'In', 'Pi', 'San', 'Z', 'Soi', 'Katri', 'Gi', 'Tuuli', 'Kar', 'Sa', 'Ma', 'Johanna', 'Marja', 'Di'], ['Mar', '-', 'Maria', 'San', 'Mari', 'Ra', 'I', 'Ri', 'Sol', 'Ali', 'Car', 'Sar', 'Angel', 'Gu', 'Min', 'Di', 'Sari', '.', 'Al', 'Kar', '[UNK]', 'Ba', 'Ran', 'Pa', 'Nu', 'Can', 'Mor', 'Re', 'Ga', 'Meri', 'Nor', 'Fe', 'Len', 'Lo', ',', 'Var', 'Ama', 'Ma', 'Pen', 'Es', 'Lin', 'Ver', 'De', 'Er', 'An', 'Am', 'Jo', 'Nan', 'Mer', 'Kir', 'Na', 'Tur', 'Ro', 'Bon', 'Bar', 'de', 'Ham', 'Sal', 'Mai', 'Mi', 'Tan', 'Cas', 'Ta', 'Sa', 'El', 'Man', 'Marie', 'No', 'Mil', 'Ni', 'Rosa', 'Eli', 'Lau', 'Sel', 'Marian', 'Christ', 'Roman', 'Jar', 'Cer', 'ja', 'In', 'Marin', 'Per', \"'\", 'Pe', 'Pol', 'Ven', 'Ag', 'Reg', 'Vi', 'Sor', 'Mat', 'Her', 'Lat', 'Za', 'Franc', 'Ur', 'Os', 'Mel', 'Ing'], [',', 'Lehtonen', 'Mäkelä', 'Nieminen', 'Aalto', 'Vainio', 'Lehto', 'Mäkinen', 'Virtanen', 'Aaltonen', 'Koskinen', 'Korhonen', 'Hämäläinen', 'Kallio', 'Manninen', 'Niemi', 'Turunen', 'Salonen', 'Salminen', 'Toivonen', 'Kataja', 'Salmi', 'Heikkilä', 'Aro', 'Tuominen', 'Peltola', 'Rautio', 'Ikonen', 'Laitinen', 'Nyman', 'Jokinen', 'Saarinen', 'Kangas', 'Leino', 'Poutiainen', 'Heinonen', 'Huttunen', 'Koski', 'Laine', 'Lehtinen', 'Lassila', 'Ahonen', 'Mikkonen', 'Peltonen', 'Heiskanen', 'Harju', 'Niskanen', 'Seppälä', 'Huovinen', 'Mikkola', 'Karjalainen', 'Sorsa', 'Kettunen', 'Järvinen', 'Uotila', 'Hiltunen', 'Kantola', 'Mäki', 'Mattila', 'Pulkkinen', 'Jansson', 'Heikkinen', 'Aho', 'Heino', 'Paananen', 'Salo', 'Savolainen', 'Härkönen', 'Varis', 'Halonen', 'Rossi', 'Lahtinen', 'Kärki', 'Helin', '##la', 'Väisänen', 'Laakso', 'Uusitalo', 'Kuusisto', 'Pietilä', 'Nykänen', 'Vuorinen', '##kka', 'Jääskeläinen', 'Penttilä', 'Miettinen', 'Linna', 'Häkkinen', 'Alanko', 'Mäenpää', 'Leinonen', 'Murto', 'Rinne', 'Asikainen', 'Snellman', 'Halme', 'Hirvonen', 'Susi', 'Anttila', 'Määttä']]]}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "111b2c62-dbe2-423d-aa0d-9dd8fe1f1156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marjukka', 'Ri', 'Ra', 'E', 'El', 'Vil', 'Ti', 'Jo', 'I', 'An', 'J', 'Ki', 'Kari', 'Jani', 'La', 'Ro', 'Aa', 'Ne', 'Pe', 'Un', 'Jan', 'Mari', 'Te', 'Mai', 'Mi', 'O', 'Ai', 'Se', 'Eli', 'Meri', 'Erik', 'As', 'Juha', 'Aar', 'Isa', 'Vi', 'Kä', 'Li', 'Ir', 'Al', 'Lil', 'Antti', 'Mati', 'Ni', 'Y', 'Per', 'Her', 'Satu', 'S', 'Pa', 'C', 'Matti', 'Ly', 'U', 'Var', 'Jukka', 'Jorma', 'Ei', 'Za', 'Tor', 'F', 'Ari', 'En', 'Lu', 'G', 'Hei', 'Elli', 'Lauri', 'Rei', 'Es', 'Er', 'W', 'Ha', 'Kas', 'Ta', 'Pekka', 'M', 'Na', 'Pir', 'Anna', 'Val', 'Ts', 'Tiina', 'Janne', 'Martti', 'Mar', 'Timo', 'In', 'Pi', 'San', 'Z', 'Soi', 'Katri', 'Gi', 'Tuuli', 'Kar', 'Sa', 'Ma', 'Johanna', 'Marja', 'Di']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Te\n"
     ]
    }
   ],
   "source": [
    "to_redact_words = output[\"to_redact_words\"]\n",
    "possible_redactions = output[\"predictions\"]\n",
    "input = output[\"tokenizer_output\"]\n",
    "\n",
    "def find_best_sim(sims, word_index=0):\n",
    "    # word index == 0 => we are finding matches to the original word\n",
    "    for sim in sims:\n",
    "        if sim[0] == word_index:\n",
    "            return sim\n",
    "    return False\n",
    "\n",
    "\n",
    "for words, preds in zip(to_redact_words, possible_redactions):\n",
    "    result = \"\"\n",
    "    for i, w, p in zip(range(len(words)),words, preds):\n",
    "        if i == 0: # first iteration\n",
    "            v = [\"\".join([w_.replace(\"##\",\"\") for w_ in words])]+p\n",
    "            print(v)\n",
    "            emb=get_embed_for_one_instance(v, pipe)\n",
    "            sims = cosine_sim(emb,emb)\n",
    "            w_sim = find_best_sim(sims)\n",
    "            assert w_sim is not False\n",
    "            print(p[w_sim[1]])\n",
    "            result = p[w_sim[1]]\n",
    "        else:\n",
    "            \n",
    "            \n",
    "\n",
    "#to_redict = tok.input_ids[0][**redact_ids]\n",
    "#to_substitute = tokenizer.decode(to_redact)\n",
    "#good_predictions_1 = [\"Mai\", \"An\", \"Emi\", \"Luci\"]\n",
    "#choose_index = 1 # An\n",
    "#print(to_substitute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81785e0e-26a9-4607-93a9-5eead28b7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embed\n",
    "orig = get_embed_for_one_instance(to_substitute)\n",
    "guesses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b3c9b-f578-4a07-ae98-c2d0b52d84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768e65b-d8ba-4469-b6c3-d16fc8f3cb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
