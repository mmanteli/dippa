{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524f8052-fcaf-47e8-b0fb-33dd87eb35cd",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Redaction of PII in these steps:\n",
    "\n",
    "- Select $n$ most probable subtitutions\n",
    "- from these, extract $k$ most similar in terms of cosine similarity (hyper-sphere)\n",
    "- select randomly\n",
    "\n",
    "For multisubtoken words, nothing is said(?).\n",
    "=> Trying by finding the most similar to the entire word. E.g.\n",
    "\n",
    "- original = contex1 + \"Amanda\" + context2\n",
    "- tokenized = \"Aman\" \"da\"\n",
    "- redaction result = yes\n",
    "- subtitution by token:\n",
    "1. for context1 + [MASK] [MASK] + context2 the most probable predictions for the **first masked** are $S_1$\n",
    "2. Find $S^r_1$ closest to \"Amanda\" where \"Amanda\" is max pooled form \"Aman\" and \"da\"\n",
    "3. select $s_1$ in $S^r_1$ randomly\n",
    "4. $S_2$ == most probable for context + s_1 + [MASK] + context2\n",
    "5. Similarly find one closest to \"Amanda\" => s_2\n",
    "6. Assume $s_1$+$s_2$ is a coherent word :)\n",
    "\n",
    "Notes:\n",
    "\n",
    "- if using the same model to redact and find substitutions: less computation\n",
    "- if using different models, you may optimize both\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6fdbf2-6cbd-44a8-a165-8ec42716da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, pipelines, AutoTokenizer, AutoModelForPreTraining\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58268bbd-d40e-4c23-ba9a-78b0b980d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from piifinder import PiiFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666182e9-ea98-49e2-9151-4a300d634051",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"TurkuNLP/bert-base-finnish-cased-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ad8a3c-e5e0-456b-9913-864f64097331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking these from past course work:\n",
    "\n",
    "def get_embed_for_full_dataset(dataset, lang, pipeline):\n",
    "  model_name = MODEL_NAME\n",
    "  #p=pipeline(task=\"feature-extraction\",model=model_name,return_tensors=True,device=0)\n",
    "  embedded=pipeline(pipelines.pt_utils.KeyDataset(dataset[lang], \"text\"), batch_size=64, truncation=\"only_first\")\n",
    "\n",
    "  # to cpu and take the mean over words\n",
    "  embedded_pooled=[torch.mean(elem,axis=1).cpu() for elem in embedded]\n",
    "  # to single matrix\n",
    "  results=torch.vstack(embedded_pooled).numpy()\n",
    "  return results\n",
    "\n",
    "def get_embed_for_one_instance(x, pipeline):\n",
    "    #p=pipeline(task=\"feature-extraction\",model=MODEL_NAME,return_tensors=True,device=0)\n",
    "    return pipeline(x)\n",
    "\n",
    "def cosine_sim(x,y):\n",
    "  M=cosine_similarity(x,y)\n",
    "  aligned=np.argsort(-M,axis=-1)\n",
    "\n",
    "  sims=[]\n",
    "  for i in range(M.shape[0]): #M.shape[0] is the number of rows / input documents\n",
    "    j=aligned[i,0] # index 1 for 2nd best match => first one if different languages\n",
    "    score=M[i,j]\n",
    "    sims.append((i,j,score))\n",
    "  # sort in descending order  element -> score => sort by score\n",
    "  sims.sort(key=lambda element:element[2],reverse=True)\n",
    "\n",
    "  return sims\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b829b348-4396-4c76-9055-b9266d2f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is Leena and I like playing the piano\"\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "pipe = pipeline(task=\"feature-extraction\",model=MODEL_NAME,return_tensors=True,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9586dc21-0dbd-401b-b35c-44a3d6326105",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PiiFinder(model, tokenizer, 1e-4, \"WordPiece\")\n",
    "output = pf.find_pii(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cd730e-c309-469d-adfe-ddd840ada6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoded_text': '[CLS] My name is Leena and I like playing the piano [SEP]', 'tokenizer_output': {'input_ids': tensor([[  102,  9562, 37932, 50010,   645, 11266,  9624,   320, 21267, 50010,\n",
      "         18463,  1365,  7217, 37837,   103]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'to_redact': [[5], [10, 11]]}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111b2c62-dbe2-423d-aa0d-9dd8fe1f1156",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (861437718.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    to_redict = tok.input_ids[0][**redact_ids]\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "to_redact = output[\"to_redact\"]\n",
    "input = output[\"tokenizer_ouput\"]\n",
    "\n",
    "#to_redict = tok.input_ids[0][**redact_ids]\n",
    "#to_substitute = tokenizer.decode(to_redact)\n",
    "#good_predictions_1 = [\"Mai\", \"An\", \"Emi\", \"Luci\"]\n",
    "#choose_index = 1 # An\n",
    "#print(to_substitute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81785e0e-26a9-4607-93a9-5eead28b7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embed\n",
    "orig = get_embed_for_one_instance(to_substitute)\n",
    "guesses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b3c9b-f578-4a07-ae98-c2d0b52d84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768e65b-d8ba-4469-b6c3-d16fc8f3cb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
