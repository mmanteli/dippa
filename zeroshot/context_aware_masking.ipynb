{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a21bc4-cd0a-4c21-8994-98e06bbc13c8",
   "metadata": {},
   "source": [
    "# Problem with context:\n",
    "\n",
    "If the sentence contains two of the same name, then problems start to occur: we want to mask both Amanda's in the following text:\n",
    "\n",
    "    My name is Amanda and you email me at amanda@email.com\n",
    "\n",
    "Problem 2: \"Amanda\" and \"amanda\" are tokenized differently, i.e.:\n",
    "\n",
    "    Amanda = Aman ##da\n",
    "    amanda = a ##man ##da\n",
    "\n",
    "How to combine these...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d4943f-865f-4cf2-8f87-cf35658a0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874d83c4-21cc-4fdd-a464-e653a4979de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForPreTraining.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ddbf0d-684d-4c05-b4c3-a2f7beb5da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "special_tokens = tokenizer.all_special_tokens\n",
    "print(special_tokens)\n",
    "continuation_marker = \"##\"   # how to get this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd58ce6-7d07-4e47-923f-d36220071629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='TurkuNLP/bert-base-finnish-cased-v1', vocab_size=50105, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "[<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, <class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'>, <class 'transformers.tokenization_utils_base.SpecialTokensMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'object'>]\n",
      "['BertTokenizer', 'BertTokenizerFast', 'List', 'Optional', 'PRETRAINED_INIT_CONFIGURATION', 'PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES', 'PRETRAINED_VOCAB_FILES_MAP', 'PreTrainedTokenizerFast', 'Tuple', 'VOCAB_FILES_NAMES', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'json', 'logger', 'logging', 'normalizers']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.__class__.mro())\n",
    "print(dir(transformers.models.bert.tokenization_bert_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90de0298-a2bc-4d52-b627-a93e525106d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def mask(text, tokenizer):\n",
    "\n",
    "    def get_indices(t):\n",
    "        converted = tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            if converted[i][:2] != continuation_marker and converted[i] not in special_tokens:\n",
    "                indices.append([i])\n",
    "            else:\n",
    "                if converted[i] not in special_tokens and indices!=[]:   # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "        return indices   \n",
    "    \n",
    "    t = tokenizer(text, return_tensors='pt') # prepare normal tokenized input\n",
    "    indices = get_indices(t)\n",
    "\n",
    "    return indices, t, tokenizer.decode(t.input_ids[0])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_same_tokens(lst, index):\n",
    "    target_value = lst[index]\n",
    "    return [i for i, value in enumerate(lst) if value == target_value and i != index]\n",
    "\n",
    "\n",
    "#def find_indices_of_same_word(lst, item):\n",
    "#    return [i for i, x in enumerate(lst) if x == item]\n",
    "\n",
    "\n",
    "\n",
    "def context_aware_mask(text, tokenizer):\n",
    "\n",
    "    def get_indices(t):\n",
    "        converted = tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        words = []\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            if converted[i][:2] != continuation_marker and converted[i] not in special_tokens:\n",
    "                indices.append([i])\n",
    "                words.append(converted[i].lower())\n",
    "            else:\n",
    "                if converted[i] not in special_tokens and indices!=[]:   # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "                    words[-1] += converted[i][2:].lower()\n",
    "\n",
    "        indices_context=[]\n",
    "        assert len(words)==len(indices), \"Issues with masking the sentence.\"\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            ind_of_words = find_same_tokens(words, i)\n",
    "            if ind_of_words != []:\n",
    "                #print(words[i],\":\", ind_of_words, np.array(words)[ind_of_words])\n",
    "                current = []\n",
    "                for j in ind_of_words:\n",
    "                    current+= indices[j]\n",
    "                indices_context.append(current)\n",
    "            else:\n",
    "                indices_context.append([])\n",
    "        \n",
    "        assert len(indices)==len(indices_context), \"Issues with context masking, \"+str(len(indices))+\"!=\"+str(len(indices_context))+\"\\nIndices:\\t\"+str(indices)+\"\\nContext:\\t\"+str(indices_context)\n",
    "        return indices, indices_context  \n",
    "    \n",
    "    t = tokenizer(text, return_tensors='pt') # prepare normal tokenized input\n",
    "    \n",
    "    indices, indices_context = get_indices(t)\n",
    "\n",
    "    return indices, t, tokenizer.decode(t.input_ids[0]), indices_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fbf58a-0cd8-45f3-b287-7725f1a4877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[2] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[3] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[4, 5] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[6] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[7] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[8] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[9] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[10] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[11] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[12, 13, 14] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[15] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[16] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[17] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[18, 19, 20] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[21] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[22, 23] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[24] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[25] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "[[], [6], [], [12, 13, 14], [2], [], [], [], [], [], [4, 5], [17, 24], [], [15, 24], [], [], [], [15, 17], []]\n",
      "[[1], [2], [3], [4, 5], [6], [7], [8], [9], [10], [11], [12, 13, 14], [15], [16], [17], [18, 19, 20], [21], [22, 23], [24], [25]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda.a.myntti@utu.fi\"\n",
    "masked_indices, tokenized_text, decoded_text, context = context_aware_mask(text, tokenizer)\n",
    "for i, c in zip(masked_indices, context):\n",
    "    print(i, decoded_text)#,tokenized_text)\n",
    "print(context)\n",
    "print(masked_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b198c6b3-9c4d-4c1e-b3a1-b5992487d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_probability(A):\n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    return softmax(A)\n",
    "\n",
    "def predict(masked, i, true_token, print_results=False, top=10):\n",
    "    # do a prediction\n",
    "    model_out = model(**masked)\n",
    "    logits = model_out[\"prediction_logits\"]\n",
    "\n",
    "    # logits for this word specifically\n",
    "    logits_i = logits[0,i,:]  # this contains the probabilities for this token\n",
    "    # change to probability\n",
    "    probs = to_probability(logits_i)\n",
    "    # true token is the index\n",
    "    word_probability = probs[true_token]\n",
    "\n",
    "    # Do only in debug mode:\n",
    "    if print_results:\n",
    "        print(f'{tokenizer.decode(true_token)} has probability {word_probability}')\n",
    "        # see 10 top predictions for debug\n",
    "        top_logits, top_tokens= torch.sort(logits, dim=2, descending=True)#[:,:,:top]\n",
    "        top_probs = to_probability(top_logits[0,i,:])\n",
    "        top_logits = top_logits[:,:,:top]\n",
    "        top_tokens = top_tokens[:,:,:top]\n",
    "\n",
    "    \n",
    "        print(\"Guesses:\",tokenizer.decode(top_tokens[0,i,:]))\n",
    "        print(\"Logits: \",top_logits[0,i,:])\n",
    "        print(\"Probs:  \",top_probs[:top])\n",
    "    return word_probability\n",
    "\n",
    "\n",
    "def get_scores(to_be_masked, tokens, context=[], debug=False):\n",
    "    \"\"\"\n",
    "    Calculates the (aggregated) probability of the given word based on the model prediction.\n",
    "    For multi-subtoken words, aggregation strategy is gradual unmasking and multiplication.\n",
    "    Input: \n",
    "        tokens: tokenizer output for a span of text\n",
    "        to_be_masked: indices for which are masked from the tokens and over which we calculate\n",
    "                      i.e. indices of the subtokens that form a word.\n",
    "        debug (False): prints out extra information if True\n",
    "    Returns:\n",
    "        (aggregated) probability \\in (0,1)\n",
    "    \"\"\"\n",
    "    # initialize the score; we're multiplying, so 1\n",
    "    final_score = 1\n",
    "\n",
    "    # loop over the subtokens of a word\n",
    "    for i in range(len(to_be_masked)):\n",
    "        # making a deep copy as tensors are nested and yada yada\n",
    "        t = copy.deepcopy(tokens)\n",
    "        current = to_be_masked[i:]   # this is the token we are CURRENTLY interested in\n",
    "        for j in current:\n",
    "            t[\"input_ids\"][0][j] = tokenizer.mask_token_id     # we mask the SUBtokens that are in current\n",
    "        if context != []:\n",
    "            for j in context:\n",
    "                t[\"input_ids\"][0][j] = tokenizer.mask_token_id \n",
    "        if debug:\n",
    "            print(tokenizer.decode(t[\"input_ids\"][0]))\n",
    "        # multiply the final score with the predicted probability => aggregates over to_be_masked==one word\n",
    "        final_score *= predict(t, current[0], tokens.input_ids[0][current[0]], print_results=debug)\n",
    "        \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fb3bc0-00be-487a-99b5-c7db6e654560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK], olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "Moi has probability 0.09783897548913956\n",
      "Guesses: Hei Moi Niin Muuten Juu hei Joo Minä Eli Kiitos\n",
      "Logits:  tensor([18.0196, 16.0267, 14.7276, 14.0297, 13.9221, 13.8744, 13.7270, 13.6844,\n",
      "        13.5303, 13.0911], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.7178, 0.0978, 0.0267, 0.0133, 0.0119, 0.0114, 0.0098, 0.0094, 0.0081,\n",
      "        0.0052], grad_fn=<SliceBackward0>)\n",
      "Moi \t >> 0.09783897548913956\n",
      "[CLS] Moi [MASK] olen Amanda [MASK] mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      ", has probability 0.9052180051803589\n",
      "Guesses: ,! ja : mä minäkka Moi mäkin itse\n",
      "Logits:  tensor([18.4248, 15.2571, 14.5421, 12.8983, 12.7908, 12.5602, 12.4797, 12.1201,\n",
      "        11.8746, 11.7475], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.9052, 0.0381, 0.0186, 0.0036, 0.0032, 0.0026, 0.0024, 0.0017, 0.0013,\n",
      "        0.0011], grad_fn=<SliceBackward0>)\n",
      ", \t >> 0.9052180051803589\n",
      "[CLS] Moi, [MASK] Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "olen has probability 0.05866984277963638\n",
      "Guesses: Hei hei Moi olen ihana rakas sinä kiitos oon ja\n",
      "Logits:  tensor([14.1292, 14.0475, 13.0499, 12.8344, 12.2256, 12.0267, 11.9355, 11.8929,\n",
      "        11.7187, 11.6651], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.2142, 0.1974, 0.0728, 0.0587, 0.0319, 0.0262, 0.0239, 0.0229, 0.0192,\n",
      "        0.0182], grad_fn=<SliceBackward0>)\n",
      "olen \t >> 0.05866984277963638\n",
      "[CLS] Moi, olen [MASK] [MASK], mulle voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK]. a. myntti @ utu. fi [SEP]\n",
      "Aman has probability 0.0006089723319746554\n",
      "Guesses: kiinnostunut varasi ihan jo täällä siis vasta kyseli aivan opiskelija\n",
      "Logits:  tensor([9.7514, 8.6272, 8.5790, 8.3920, 8.3703, 8.3113, 8.2975, 7.7081, 7.6859,\n",
      "        7.5638], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.0443, 0.0144, 0.0137, 0.0114, 0.0111, 0.0105, 0.0103, 0.0057, 0.0056,\n",
      "        0.0050], grad_fn=<SliceBackward0>)\n",
      "[CLS] Moi, olen Aman [MASK], mulle voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK]. a. myntti @ utu. fi [SEP]\n",
      "##da has probability 0.00753325829282403\n",
      "Guesses: takana jäsen kirjoittaja sisko tyttö tytär äiti poika kanssa fani\n",
      "Logits:  tensor([11.1711, 10.9711, 10.9189, 10.6735, 10.5911, 10.5891, 10.5467, 10.4128,\n",
      "        10.3405,  9.9043], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.0491, 0.0402, 0.0382, 0.0299, 0.0275, 0.0274, 0.0263, 0.0230, 0.0214,\n",
      "        0.0138], grad_fn=<SliceBackward0>)\n",
      "Amanda \t >> 4.587545845424756e-06\n",
      "[CLS] Moi [MASK] olen Amanda [MASK] mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      ", has probability 0.2064819186925888\n",
      "Guesses: ja, eli mutta joten niin! mut -.\n",
      "Logits:  tensor([18.1379, 16.8386, 13.4037, 13.1458, 13.0205, 12.5789, 12.2754, 12.0972,\n",
      "        11.8322, 11.6748], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.7571, 0.2065, 0.0067, 0.0051, 0.0045, 0.0029, 0.0022, 0.0018, 0.0014,\n",
      "        0.0012], grad_fn=<SliceBackward0>)\n",
      ", \t >> 0.2064819186925888\n",
      "[CLS] Moi, olen Amanda, [MASK] voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "mulle has probability 0.005366495344787836\n",
      "Guesses: joten ja mutta eli minulle niin minusta nyt mulle mut\n",
      "Logits:  tensor([17.9620, 16.8124, 16.7235, 15.9874, 15.7551, 15.1396, 14.0246, 13.6029,\n",
      "        13.4692, 13.4233], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.4796, 0.1519, 0.1390, 0.0666, 0.0528, 0.0285, 0.0094, 0.0061, 0.0054,\n",
      "        0.0051], grad_fn=<SliceBackward0>)\n",
      "mulle \t >> 0.005366495344787836\n",
      "[CLS] Moi, olen Amanda, mulle [MASK] laittaa viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "voit has probability 0.1366426795721054\n",
      "Guesses: voi voit saa kannattaa voitte voisi voipi voisit voin pitää\n",
      "Logits:  tensor([21.0307, 19.3314, 18.5237, 16.9238, 16.8049, 16.3596, 15.9170, 15.8855,\n",
      "        15.6609, 15.6442], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.7475, 0.1366, 0.0609, 0.0123, 0.0109, 0.0070, 0.0045, 0.0044, 0.0035,\n",
      "        0.0034], grad_fn=<SliceBackward0>)\n",
      "voit \t >> 0.1366426795721054\n",
      "[CLS] Moi, olen Amanda, mulle voit [MASK] viestiä osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "laittaa has probability 0.876126229763031\n",
      "Guesses: laittaa lähettää pistää heittää jättää kirjoittaa laitta laitaa laita kirjoitella\n",
      "Logits:  tensor([22.4297, 20.0312, 18.8381, 17.6209, 17.3614, 16.2856, 15.7287, 15.2897,\n",
      "        14.8128, 14.2416], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([8.7613e-01, 7.9597e-02, 2.4140e-02, 7.1471e-03, 5.5136e-03, 1.8803e-03,\n",
      "        1.0773e-03, 6.9456e-04, 4.3111e-04, 2.4350e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "laittaa \t >> 0.876126229763031\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa [MASK] osoitteeseen amanda. a. myntti @ utu. fi [SEP]\n",
      "viestiä has probability 0.10039646923542023\n",
      "Guesses: sähköpostia postia viestiä viestin kysymyksiä palautetta viestejä sähköpostin osoitteen terveisiä\n",
      "Logits:  tensor([20.9638, 19.5079, 19.0048, 16.3507, 14.9418, 14.8704, 14.5439, 14.2945,\n",
      "        14.1565, 14.1186], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.7120, 0.1660, 0.1004, 0.0071, 0.0017, 0.0016, 0.0012, 0.0009, 0.0008,\n",
      "        0.0008], grad_fn=<SliceBackward0>)\n",
      "viestiä \t >> 0.10039646923542023\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä [MASK] amanda. a. myntti @ utu. fi [SEP]\n",
      "osoitteeseen has probability 0.3348371684551239\n",
      "Guesses: : osoitteeseen sähköpostilla,. sähköpostitse sähköpostiin mulle! osoitteessa\n",
      "Logits:  tensor([17.5481, 17.1322, 15.3608, 14.2926, 13.4750, 13.4220, 13.3789, 13.1901,\n",
      "        12.9349, 12.7685], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.5075, 0.3348, 0.0570, 0.0196, 0.0086, 0.0082, 0.0078, 0.0065, 0.0050,\n",
      "        0.0043], grad_fn=<SliceBackward0>)\n",
      "osoitteeseen \t >> 0.3348371684551239\n",
      "[CLS] Moi, olen [MASK] [MASK], mulle voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK]. a. myntti @ utu. fi [SEP]\n",
      "am has probability 0.001443810062482953\n",
      "Guesses: : an mi jaa mar han w ma j kat\n",
      "Logits:  tensor([11.1028, 10.2867, 10.1051, 10.0236,  9.8920,  9.7897,  9.7728,  9.7662,\n",
      "         9.6710,  9.4867], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.0628, 0.0277, 0.0231, 0.0213, 0.0187, 0.0169, 0.0166, 0.0165, 0.0150,\n",
      "        0.0125], grad_fn=<SliceBackward0>)\n",
      "[CLS] Moi, olen [MASK] [MASK], mulle voit laittaa viestiä osoitteeseen am [MASK] [MASK]. a. myntti @ utu. fi [SEP]\n",
      "##and has probability 4.0982609789352864e-05\n",
      "Guesses: . - : am / an at @ o _\n",
      "Logits:  tensor([15.6690, 13.3050, 11.9250, 11.6486, 11.2807, 10.6975, 10.4025, 10.2034,\n",
      "        10.0388,  9.9772], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.7597, 0.0714, 0.0180, 0.0136, 0.0094, 0.0053, 0.0039, 0.0032, 0.0027,\n",
      "        0.0026], grad_fn=<SliceBackward0>)\n",
      "[CLS] Moi, olen [MASK] [MASK], mulle voit laittaa viestiä osoitteeseen amand [MASK]. a. myntti @ utu. fi [SEP]\n",
      "##a has probability 0.26430779695510864\n",
      "Guesses: ##aioiaeyersur\n",
      "Logits:  tensor([15.3063, 14.0008, 13.7136, 13.2489, 12.9517, 12.6692, 12.5895, 12.5454,\n",
      "        12.5238, 12.4757], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.2643, 0.0716, 0.0538, 0.0338, 0.0251, 0.0189, 0.0175, 0.0167, 0.0164,\n",
      "        0.0156], grad_fn=<SliceBackward0>)\n",
      "amanda \t >> 1.563938489823613e-08\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda [MASK] a [MASK] myntti @ utu [MASK] fi [SEP]\n",
      ". has probability 0.4732753038406372\n",
      "Guesses: . ( - [ _ +, / : @\n",
      "Logits:  tensor([20.1103, 19.9892, 17.6885, 17.5830, 17.1117, 14.0457, 13.5640, 12.9566,\n",
      "        12.5978, 12.3845], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([4.7328e-01, 4.1933e-01, 4.2009e-02, 3.7804e-02, 2.3596e-02, 1.0998e-03,\n",
      "        6.7936e-04, 3.7010e-04, 2.5851e-04, 2.0886e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      ". \t >> 0.4732753038406372\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. [MASK]. myntti @ utu. fi [SEP]\n",
      "a has probability 0.10224533081054688\n",
      "Guesses: o a s m j k d c al af\n",
      "Logits:  tensor([12.7441, 12.4831, 12.0245, 11.4933, 11.2575, 10.8913, 10.7902, 10.6023,\n",
      "        10.4728, 10.4695], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.1327, 0.1022, 0.0646, 0.0380, 0.0300, 0.0208, 0.0188, 0.0156, 0.0137,\n",
      "        0.0137], grad_fn=<SliceBackward0>)\n",
      "a \t >> 0.10224533081054688\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda [MASK] a [MASK] myntti @ utu [MASK] fi [SEP]\n",
      ". has probability 0.14702488481998444\n",
      "Guesses: - )., _ / ja tai ] (\n",
      "Logits:  tensor([18.3562, 17.5548, 17.1314, 15.8443, 15.7132, 14.9993, 14.4329, 13.9148,\n",
      "        13.7591, 12.8234], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.5004, 0.2245, 0.1470, 0.0406, 0.0356, 0.0174, 0.0099, 0.0059, 0.0050,\n",
      "        0.0020], grad_fn=<SliceBackward0>)\n",
      ". \t >> 0.14702488481998444\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. [MASK] [MASK] [MASK] @ utu. fi [SEP]\n",
      "my has probability 0.0004871261480730027\n",
      "Guesses: w la s o ha an jaa lin san ma\n",
      "Logits:  tensor([10.1935,  9.5416,  9.4600,  9.4512,  9.3181,  9.2020,  9.1773,  8.9943,\n",
      "         8.9673,  8.9448], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.0273, 0.0142, 0.0131, 0.0130, 0.0114, 0.0101, 0.0099, 0.0082, 0.0080,\n",
      "        0.0078], grad_fn=<SliceBackward0>)\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. my [MASK] [MASK] @ utu. fi [SEP]\n",
      "##nt has probability 0.0018640542402863503\n",
      "Guesses: ##lnlidlogtylinsw\n",
      "Logits:  tensor([12.7389, 12.3771, 12.0489, 11.9371, 11.7965, 11.7569, 11.6653, 11.6630,\n",
      "        11.6402, 11.6149], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.0339, 0.0236, 0.0170, 0.0152, 0.0132, 0.0127, 0.0116, 0.0116, 0.0113,\n",
      "        0.0110], grad_fn=<SliceBackward0>)\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. mynt [MASK] @ utu. fi [SEP]\n",
      "##ti has probability 0.007376472465693951\n",
      "Guesses: ##oaieniuseronilaolae\n",
      "Logits:  tensor([16.5169, 16.4019, 15.7852, 15.7205, 15.5410, 15.0099, 14.9506, 14.8831,\n",
      "        14.8074, 14.7720], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.1130, 0.1007, 0.0544, 0.0509, 0.0426, 0.0250, 0.0236, 0.0220, 0.0204,\n",
      "        0.0197], grad_fn=<SliceBackward0>)\n",
      "myntti \t >> 6.6980549995321326e-09\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti [MASK] utu. fi [SEP]\n",
      "@ has probability 0.995269238948822\n",
      "Guesses: @ at - ät. ) (, _ /\n",
      "Logits:  tensor([24.3747, 18.0933, 17.4172, 17.2484, 17.1045, 15.7489, 14.8826, 14.1721,\n",
      "        13.7406, 13.4386], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([9.9527e-01, 1.8621e-03, 9.4698e-04, 7.9994e-04, 6.9272e-04, 1.7857e-04,\n",
      "        7.5092e-05, 3.6899e-05, 2.3968e-05, 1.7721e-05],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "@ \t >> 0.995269238948822\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ [MASK] [MASK]. fi [SEP]\n",
      "ut has probability 0.014398469589650631\n",
      "Guesses: suomi kolum ilme ut j ni sauna ms y mtv\n",
      "Logits:  tensor([15.2530, 13.2150, 12.4565, 11.5890, 11.4684, 11.4553, 11.3629, 11.2194,\n",
      "        11.2013, 10.9273], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.5618, 0.0732, 0.0343, 0.0144, 0.0128, 0.0126, 0.0115, 0.0099, 0.0098,\n",
      "        0.0074], grad_fn=<SliceBackward0>)\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ ut [MASK]. fi [SEP]\n",
      "##u has probability 0.5886359214782715\n",
      "Guesses: ##uaevaeeraeduyeliuvaku\n",
      "Logits:  tensor([20.4216, 19.3831, 17.0023, 16.9497, 16.4506, 16.2799, 16.1624, 15.6661,\n",
      "        15.5352, 15.4423], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([0.5886, 0.2084, 0.0193, 0.0183, 0.0111, 0.0094, 0.0083, 0.0051, 0.0044,\n",
      "        0.0040], grad_fn=<SliceBackward0>)\n",
      "utu \t >> 0.00847545638680458\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda [MASK] a [MASK] myntti @ utu [MASK] fi [SEP]\n",
      ". has probability 0.9999898672103882\n",
      "Guesses: . -, fi : piste _ / [UNK]?\n",
      "Logits:  tensor([27.7588, 14.8967, 14.4258, 14.0359, 14.0104, 13.6687, 13.6122, 13.4695,\n",
      "        12.6693, 12.5141], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([9.9999e-01, 2.5945e-06, 1.6201e-06, 1.0970e-06, 1.0694e-06, 7.5986e-07,\n",
      "        7.1810e-07, 6.2261e-07, 2.7969e-07, 2.3951e-07],\n",
      "       grad_fn=<SliceBackward0>)\n",
      ". \t >> 0.9999898672103882\n",
      "[CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda. a. myntti @ utu. [MASK] [SEP]\n",
      "fi has probability 0.9912620782852173\n",
      "Guesses: fi net com info edu org f nu ut ru\n",
      "Logits:  tensor([21.6515, 15.9073, 15.2113, 13.9613, 13.7729, 13.6632, 13.4043, 13.3931,\n",
      "        13.1256, 13.0009], grad_fn=<SliceBackward0>)\n",
      "Probs:   tensor([9.9126e-01, 3.1734e-03, 1.5822e-03, 4.5333e-04, 3.7547e-04, 3.3645e-04,\n",
      "        2.5971e-04, 2.5680e-04, 1.9653e-04, 1.7350e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "fi \t >> 0.9912620782852173\n"
     ]
    }
   ],
   "source": [
    "for ind, cont in zip(masked_indices, context):\n",
    "    final_score = get_scores(ind, tokenized_text, context = cont, debug=True)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    print(f'{word} \\t >> {final_score}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537391f-7d62-43f5-9585-630f2ab0f04f",
   "metadata": {},
   "source": [
    " ## Actual function \n",
    "\n",
    " Also testing the hypothesis that email screws names over..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4f76cf-02e6-44e5-891c-5f634ecc5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moi \t >> 0.11190123856067657\n",
      ", \t >> 0.8906912803649902\n",
      "olen \t >> 0.09335323423147202\n",
      "Amanda \t >> 2.389621840848122e-06 \t >> Redact\n",
      ", \t >> 0.20804329216480255\n",
      "mulle \t >> 0.00788317620754242\n",
      "voit \t >> 0.12527547776699066\n",
      "laittaa \t >> 0.8456868529319763\n",
      "viestiä \t >> 0.15869389474391937\n",
      "osoitteeseen \t >> 0.5966542363166809\n",
      "example \t >> 6.858204004700497e-10 \t >> Redact\n",
      "@ \t >> 0.995366096496582\n",
      "outlook \t >> 3.7645963857357856e-06 \t >> Redact\n",
      ". \t >> 0.9999146461486816\n",
      "com \t >> 0.9127006530761719\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-4\n",
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example@outlook.com\"\n",
    "masked_indices, tokenized_text, decoded_text, context = context_aware_mask(text, tokenizer)\n",
    "for ind, cont in zip(masked_indices, context):\n",
    "    final_score = get_scores(ind, tokenized_text, context=cont, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    if final_score < threshold:\n",
    "        print(f'{word} \\t >> {final_score} \\t >> Redact')\n",
    "    else:\n",
    "        print(f'{word} \\t >> {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebbebce5-2b37-4088-a895-087fdfdf6218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moi \t >> 0.11406373977661133\n",
      ", \t >> 0.8929223418235779\n",
      "olen \t >> 0.09167784452438354\n",
      "Amanda \t >> 7.73802000253454e-08 \t >> Redact\n",
      ", \t >> 0.209928959608078\n",
      "mulle \t >> 0.008461282588541508\n",
      "voit \t >> 0.12811265885829926\n",
      "laittaa \t >> 0.8577250242233276\n",
      "viestiä \t >> 0.15111681818962097\n",
      "osoitteeseen \t >> 0.5203199982643127\n",
      "amanda \t >> 3.7283613696370566e-12 \t >> Redact\n",
      "@ \t >> 0.996116042137146\n",
      "outlook \t >> 1.5931193502183305e-06 \t >> Redact\n",
      ". \t >> 0.9999407529830933\n",
      "com \t >> 0.8941932320594788\n"
     ]
    }
   ],
   "source": [
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda@outlook.com\"\n",
    "masked_indices, tokenized_text, decoded_text, context = context_aware_mask(text, tokenizer)\n",
    "for ind, cont in zip(masked_indices, context):\n",
    "    final_score = get_scores(ind, tokenized_text, context=cont, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    if final_score < threshold:\n",
    "        print(f'{word} \\t >> {final_score} \\t >> Redact')\n",
    "    else:\n",
    "        print(f'{word} \\t >> {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de29a5-f5bf-4ac6-a92b-9999356a9255",
   "metadata": {},
   "source": [
    "Works :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec8cda5e-535d-4528-8815-529585a04fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moi \t >> 0.09783897548913956\n",
      ", \t >> 0.9052180051803589\n",
      "olen \t >> 0.05866984277963638\n",
      "Amanda \t >> 4.587545845424756e-06 \t >> Redact\n",
      ", \t >> 0.2064819186925888\n",
      "mulle \t >> 0.005366495344787836 \t >> Redact\n",
      "voit \t >> 0.1366426795721054\n",
      "laittaa \t >> 0.876126229763031\n",
      "viestiä \t >> 0.10039646923542023\n",
      "osoitteeseen \t >> 0.3348371684551239\n",
      "amanda \t >> 1.563938489823613e-08 \t >> Redact\n",
      ". \t >> 0.4732753038406372\n",
      "a \t >> 0.10224533081054688\n",
      ". \t >> 0.14702488481998444\n",
      "myntti \t >> 6.6980549995321326e-09 \t >> Redact\n",
      "@ \t >> 0.995269238948822\n",
      "utu \t >> 0.00847545638680458 \t >> Redact\n",
      ". \t >> 0.9999898672103882\n",
      "fi \t >> 0.9912620782852173\n"
     ]
    }
   ],
   "source": [
    "threshold= 0.01\n",
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda.a.myntti@utu.fi\"\n",
    "masked_indices, tokenized_text, decoded_text, context = context_aware_mask(text, tokenizer)\n",
    "for ind, cont in zip(masked_indices, context):\n",
    "    final_score = get_scores(ind, tokenized_text, context=cont, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    if final_score < threshold:\n",
    "        print(f'{word} \\t >> {final_score} \\t >> Redact')\n",
    "    else:\n",
    "        print(f'{word} \\t >> {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee71f81-08dd-4019-8294-41a10474c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yleisimmät \t >> 0.21354475617408752\n",
      "suomenkieliset \t >> 0.0023188365157693624 \t >> Redact\n",
      "miesten \t >> 0.07334546744823456\n",
      "nimet \t >> 0.8113657236099243\n",
      "ovat \t >> 0.9380959868431091\n",
      "Matti \t >> 0.9777960181236267\n",
      "Meikäläinen \t >> 0.00024416143423877656 \t >> Redact\n",
      ", \t >> 0.9159089922904968\n",
      "Mikko \t >> 0.008687050081789494\n",
      ", \t >> 0.8539314270019531\n",
      "Tapani \t >> 0.0012471594382077456 \t >> Redact\n",
      ", \t >> 0.9592955708503723\n",
      "Ville \t >> 0.009906979277729988\n",
      ", \t >> 0.13058654963970184\n",
      "ja \t >> 0.19433386623859406\n",
      "Vladimir \t >> 0.00017671416571829468 \t >> Redact\n",
      ". \t >> 0.9940406680107117\n"
     ]
    }
   ],
   "source": [
    "threshold= 0.005\n",
    "text = \"Yleisimmät suomenkieliset miesten nimet ovat Matti Meikäläinen, Mikko, Tapani, Ville, ja Vladimir.\"\n",
    "masked_indices, tokenized_text, decoded_text, context = context_aware_mask(text, tokenizer)\n",
    "for ind, cont in zip(masked_indices, context):\n",
    "    final_score = get_scores(ind, tokenized_text, context=cont, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    if final_score < threshold:\n",
    "        print(f'{word} \\t >> {final_score} \\t >> Redact')\n",
    "    else:\n",
    "        print(f'{word} \\t >> {final_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2365bda-dd38-499c-9baa-1397f853e2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
