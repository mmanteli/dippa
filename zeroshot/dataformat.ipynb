{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a21bc4-cd0a-4c21-8994-98e06bbc13c8",
   "metadata": {},
   "source": [
    "# Now that the hypothesis is tested, I'm trying to find a good data format for the masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d4943f-865f-4cf2-8f87-cf35658a0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874d83c4-21cc-4fdd-a464-e653a4979de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForPreTraining.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ddbf0d-684d-4c05-b4c3-a2f7beb5da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "special_tokens = tokenizer.all_special_tokens\n",
    "print(special_tokens)\n",
    "continuation_marker = \"##\"   # how to get this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd58ce6-7d07-4e47-923f-d36220071629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='TurkuNLP/bert-base-finnish-cased-v1', vocab_size=50105, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "[<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>, <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, <class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'>, <class 'transformers.tokenization_utils_base.SpecialTokensMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'object'>]\n",
      "['BertTokenizer', 'BertTokenizerFast', 'List', 'Optional', 'PRETRAINED_INIT_CONFIGURATION', 'PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES', 'PRETRAINED_VOCAB_FILES_MAP', 'PreTrainedTokenizerFast', 'Tuple', 'VOCAB_FILES_NAMES', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'json', 'logger', 'logging', 'normalizers']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.__class__.mro())\n",
    "print(dir(transformers.models.bert.tokenization_bert_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90de0298-a2bc-4d52-b627-a93e525106d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mask(text, tokenizer):\n",
    "\n",
    "    def get_indices(t):\n",
    "        converted = tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            if converted[i][:2] != continuation_marker and converted[i] not in special_tokens:\n",
    "                indices.append([i])\n",
    "            else:\n",
    "                if converted[i] not in special_tokens and indices!=[]:   # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "        return indices   \n",
    "    \n",
    "    t = tokenizer(text, return_tensors='pt') # prepare normal tokenized input\n",
    "    indices = get_indices(t)\n",
    "\n",
    "    return indices, t, tokenizer.decode(t.input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d94fef0e-03c2-484f-9a4c-9b02dbe89444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[2] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[3] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[4, 5] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[6] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[7] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[8] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[9] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[10] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[11] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[12, 13, 14] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[15] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[16, 17, 18] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[19] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n",
      "[20] [CLS] Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example @ outlook. com [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example@outlook.com\"\n",
    "masked_indices, tokenized_text, decoded_text = mask(text, tokenizer)\n",
    "for i in masked_indices:\n",
    "    print(i, decoded_text)#,tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b198c6b3-9c4d-4c1e-b3a1-b5992487d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_probability(A):\n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    return softmax(A)\n",
    "\n",
    "def predict(masked, i, true_token, print_results=False, top=10):\n",
    "    # do a prediction\n",
    "    model_out = model(**masked)\n",
    "    logits = model_out[\"prediction_logits\"]\n",
    "\n",
    "    # logits for this word specifically\n",
    "    logits_i = logits[0,i,:]  # this contains the probabilities for this token\n",
    "    # change to probability\n",
    "    probs = to_probability(logits_i)\n",
    "    # true token is the index\n",
    "    word_probability = probs[true_token]\n",
    "\n",
    "    # Do only in debug mode:\n",
    "    if print_results:\n",
    "        print(f'{tokenizer.decode(true_token)} has probability {word_probability}')\n",
    "        # see 10 top predictions for debug\n",
    "        top_logits, top_tokens= torch.sort(logits, dim=2, descending=True)#[:,:,:top]\n",
    "        top_probs = to_probability(top_logits[0,i,:])\n",
    "        top_logits = top_logits[:,:,:top]\n",
    "        top_tokens = top_tokens[:,:,:top]\n",
    "\n",
    "    \n",
    "        print(\"Guesses:\",tokenizer.decode(top_tokens[0,i,:]))\n",
    "        print(\"Logits: \",top_logits[0,i,:])\n",
    "        print(\"Probs:  \",top_probs[:top])\n",
    "    return word_probability\n",
    "\n",
    "\n",
    "def get_scores(to_be_masked, tokens, debug=False):\n",
    "    \"\"\"\n",
    "    Calculates the (aggregated) probability of the given word based on the model prediction.\n",
    "    For multi-subtoken words, aggregation strategy is gradual unmasking and multiplication.\n",
    "    Input: \n",
    "        tokens: tokenizer output for a span of text\n",
    "        to_be_masked: indices for which are masked from the tokens and over which we calculate\n",
    "                      i.e. indices of the subtokens that form a word.\n",
    "        debug (False): prints out extra information if True\n",
    "    Returns:\n",
    "        (aggregated) probability \\in (0,1)\n",
    "    \"\"\"\n",
    "    # initialize the score; we're multiplying, so 1\n",
    "    final_score = 1\n",
    "\n",
    "    # loop over the subtokens of a word\n",
    "    for i in range(len(to_be_masked)):\n",
    "        # making a deep copy as tensors are nested and yada yada\n",
    "        t = copy.deepcopy(tokens)\n",
    "        current = to_be_masked[i:]   # this is the token we are CURRENTLY interested in\n",
    "        for j in current:\n",
    "            t[\"input_ids\"][0][j] = tokenizer.mask_token_id     # we mask the SUBtokens that are in current\n",
    "        if debug:\n",
    "            print(tokenizer.decode(t[\"input_ids\"][0]))\n",
    "        # multiply the final score with the predicted probability => aggregates over to_be_masked==one word\n",
    "        final_score *= predict(t, current[0], tokens.input_ids[0][current[0]], print_results=debug)\n",
    "        \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59fb3bc0-00be-487a-99b5-c7db6e654560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moi \t >> 0.11190102249383926\n",
      ", \t >> 0.914529025554657\n",
      "olen \t >> 0.09335358440876007\n",
      "Amanda \t >> 2.389645032963017e-06\n",
      ", \t >> 0.1585882604122162\n",
      "mulle \t >> 0.007883135229349136\n",
      "voit \t >> 0.1252748966217041\n",
      "laittaa \t >> 0.8456864356994629\n",
      "viestiä \t >> 0.15869447588920593\n",
      "osoitteeseen \t >> 0.5966525673866272\n",
      "example \t >> 6.858137946430531e-10\n",
      "@ \t >> 0.995366096496582\n",
      "outlook \t >> 3.76461412088247e-06\n",
      ". \t >> 0.9999146461486816\n",
      "com \t >> 0.9127005338668823\n"
     ]
    }
   ],
   "source": [
    "for ind in masked_indices:\n",
    "    final_score = get_scores(ind, tokenized_text, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    print(f'{word} \\t >> {final_score}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537391f-7d62-43f5-9585-630f2ab0f04f",
   "metadata": {},
   "source": [
    " ## Actual function \n",
    "\n",
    " Also testing the hypothesis that email screws names over..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d4f76cf-02e6-44e5-891c-5f634ecc5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moi \t >> 0.11190102249383926\n",
      ", \t >> 0.914529025554657\n",
      "olen \t >> 0.09335358440876007\n",
      "Amanda \t >> 2.389645032963017e-06 \t >> Redact\n",
      ", \t >> 0.1585882604122162\n",
      "mulle \t >> 0.007883135229349136\n",
      "voit \t >> 0.1252748966217041\n",
      "laittaa \t >> 0.8456864356994629\n",
      "viestiä \t >> 0.15869447588920593\n",
      "osoitteeseen \t >> 0.5966525673866272\n",
      "example \t >> 6.858137946430531e-10 \t >> Redact\n",
      "@ \t >> 0.995366096496582\n",
      "outlook \t >> 3.76461412088247e-06 \t >> Redact\n",
      ". \t >> 0.9999146461486816\n",
      "com \t >> 0.9127005338668823\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-4\n",
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen example@outlook.com\"\n",
    "masked_indices, tokenized_text, decoded_text = mask(text, tokenizer)\n",
    "for ind in masked_indices:\n",
    "    final_score = get_scores(ind, tokenized_text, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    if final_score < threshold:\n",
    "        print(f'{word} \\t >> {final_score} \\t >> Redact')\n",
    "    else:\n",
    "        print(f'{word} \\t >> {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ebbebce5-2b37-4088-a895-087fdfdf6218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moi \t >> 0.1140635758638382\n",
      ", \t >> 0.9137162566184998\n",
      "olen \t >> 0.09167791157960892\n",
      "Amanda \t >> 0.0007409827085211873\n",
      ", \t >> 0.1578478366136551\n",
      "mulle \t >> 0.008461365476250648\n",
      "voit \t >> 0.12811163067817688\n",
      "laittaa \t >> 0.8577243089675903\n",
      "viestiä \t >> 0.1511165201663971\n",
      "osoitteeseen \t >> 0.5203193426132202\n",
      "amanda \t >> 0.000500433670822531\n",
      "@ \t >> 0.9961161613464355\n",
      "outlook \t >> 1.5931143479974708e-06 \t >> Redact\n",
      ". \t >> 0.9999407529830933\n",
      "com \t >> 0.8941934704780579\n"
     ]
    }
   ],
   "source": [
    "text = \"Moi, olen Amanda, mulle voit laittaa viestiä osoitteeseen amanda@outlook.com\"\n",
    "masked_indices, tokenized_text, decoded_text = mask(text, tokenizer)\n",
    "for ind in masked_indices:\n",
    "    final_score = get_scores(ind, tokenized_text, debug=False)\n",
    "    word = tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "    if final_score < threshold:\n",
    "        print(f'{word} \\t >> {final_score} \\t >> Redact')\n",
    "    else:\n",
    "        print(f'{word} \\t >> {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de29a5-f5bf-4ac6-a92b-9999356a9255",
   "metadata": {},
   "source": [
    "Sliding window needed...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cda5e-535d-4528-8815-529585a04fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
