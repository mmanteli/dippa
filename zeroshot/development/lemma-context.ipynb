{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183abb14-ae90-4610-97a9-2379bf0c5925",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "\n",
    "\"Amandalle voi laittaa viestiä osoitteeseen amanda@gmail.com\"\n",
    "\n",
    "$\\rightarrow$ Amandalle ja amanda ei maskaudu oikein.\n",
    "$\\rightarrow$ Lemmatisointi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfb926b-ac89-4194-a38e-5198164c3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fi-core-news-lg==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fi_core_news_lg-3.7.0/fi_core_news_lg-3.7.0-py3-none-any.whl (230.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.8/230.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from fi-core-news-lg==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (50.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib64/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib64/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib64/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /users/mynttiam/.local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /users/mynttiam/.local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /users/mynttiam/.local/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib64/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fi-core-news-lg==3.7.0) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fi_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip -q install spacy\n",
    "! python -m spacy download fi_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9488f547-7985-481b-afe8-6cc2798dfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import string\n",
    "#from cosine_similarity import find_best_cosine_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273bc4ee-c950-4775-b21a-ab6bb969d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda\n",
      "voida\n",
      "laittaa\n",
      "viesti\n",
      "osoitteeseen\n",
      "amanda@gmail.com\n",
      ".\n",
      "viesti\n",
      "suomi\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "lem = spacy.load(\"fi_core_news_lg\")\n",
    "doc = lem(\"Amandalle voi laittaa viestiä osoitteeseen amanda@gmail.com. Viesti suomeksi.\")\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.dep_)\n",
    "    print(token.lemma_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953c7d22-bf47-48a0-8803-2d8bfdc92ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amanda', 'voida', 'laittaa', 'viesti']\n"
     ]
    }
   ],
   "source": [
    "# how about a list?\n",
    "\n",
    "lista = [\"amandalle\", \"voi\", \"laittaa\", \"viestiä\"]\n",
    "lemmatized = [t.lemma_ for t in lem(\" \".join(lista))]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6b059-1584-42be-9d64-670dfa96e435",
   "metadata": {},
   "source": [
    "### Idea: \n",
    "lemmatize at the beginning and mask a lemmatized, lowercased sentence? And at the end find overlaps?\n",
    "=> messes up witht the grammar that is essential for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433e11e-96dc-4514-b715-0a2cb70103f6",
   "metadata": {},
   "source": [
    "## Modification of the word-list in masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4866a06e-5fda-443e-9082-2ddaa40792e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiiMasker:\n",
    "\n",
    "    def __init__(self, model, tokenizer, threshold, use_context=False, choose_n=100, choose_k=3, embedding_model=None, tokenizer_type=\"BPE\", return_tokenizer_output=False):\n",
    "        self.model = model\n",
    "        self.model.eval()   # remove some unneeded functionality\n",
    "        self.tokenizer = tokenizer\n",
    "        assert tokenizer_type in [\"BPE\", \"WordPiece\"]\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.continuation_marker = {\"BPE\": \"▁\", \"WordPiece\": \"##\"}[tokenizer_type] \n",
    "        self.special_tokens = tokenizer.all_special_tokens\n",
    "        self.threshold = threshold    # this is to be optimized\n",
    "        self.use_context = bool(use_context)\n",
    "        self.choose_n = int(choose_n)\n",
    "        self.choose_k = int(choose_k)\n",
    "        self.return_tokenizer_output=return_tokenizer_output\n",
    "        if embedding_model is not None:\n",
    "            self.embedding_model = embedding_model\n",
    "        else:\n",
    "            self.embedding_model = self.model\n",
    "\n",
    "    def find_pii(self, text, debug = False):\n",
    "        masked_indices, tokenized_text, decoded_text, context = self.mask(text)\n",
    "        if context == []:\n",
    "            context = [[]*len(masked_indices)]\n",
    "        if debug: print(masked_indices)\n",
    "        to_be_redacted = []\n",
    "        to_be_redacted_words = []\n",
    "        to_redact_with = []\n",
    "        to_redact_context = []\n",
    "        for ind, cont in zip(masked_indices, context):\n",
    "            final_score, predictions = self.get_scores(ind, tokenized_text, cont, debug)\n",
    "            word = self.tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "            if final_score < self.threshold:\n",
    "                to_be_redacted.append(ind)\n",
    "                to_redact_with.append(predictions)\n",
    "                to_be_redacted_words.append(self.tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"][0][ind]))\n",
    "                to_redact_context.append(cont)\n",
    "        if self.return_tokenizer_output:\n",
    "            return {\"decoded_text\": decoded_text, \n",
    "                    \"tokenizer_output\": tokenized_text, \n",
    "                    \"to_redact_indices\": to_be_redacted, \n",
    "                    \"to_redact_words\": to_be_redacted_words, \n",
    "                    \"predictions\": to_redact_with,\n",
    "                    \"context\": to_redact_context}\n",
    "        else:\n",
    "            return {\"decoded_text\": decoded_text,\n",
    "                    \"to_redact_indices\": to_be_redacted, \n",
    "                    \"to_redact_words\": to_be_redacted_words,\n",
    "                    \"predictions\": to_redact_with,\n",
    "                    \"context\": to_redact_context}\n",
    "\n",
    "    def print_pii(self, text, debug=False):\n",
    "        masked_indices, tokenized_text, decoded_text, context = self.mask(text)\n",
    "        if context == []:\n",
    "            context = [[]*len(masked_indices)]\n",
    "        prints = []\n",
    "        for ind, cont in zip(masked_indices, context):\n",
    "            final_score, predictions = self.get_scores(ind, tokenized_text, cont, debug)\n",
    "            word = self.tokenizer.decode(tokenized_text[\"input_ids\"][0][ind])\n",
    "            if final_score < self.threshold:\n",
    "                prints.append(f'{word} \\t >> {final_score} \\t >> Redact')#, preds: {predictions[:4]}')\n",
    "            else:\n",
    "                prints.append(f'{word} \\t >> {final_score}')\n",
    "        for p in prints:\n",
    "            print(p)\n",
    "\n",
    "\n",
    "    \n",
    "#---------------------------------MASKING---------------------------------------#\n",
    "    def get_indices_WordPiece(self, t):\n",
    "        converted = self.tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            if converted[i][:2] != self.continuation_marker and converted[i] not in self.special_tokens:\n",
    "                indices.append([i])\n",
    "            else:\n",
    "                if converted[i] not in self.special_tokens and indices!=[]:   # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "        return indices\n",
    "\n",
    "    def get_indices_BPE(self, t):\n",
    "        converted = self.tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        reminder = False   # for BPE, to separate punct, we need to know if the last token was punct\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            # for BPE, continuation marker is actually a \"starting marker\"\n",
    "            if reminder or (converted[i][0] == self.continuation_marker and converted[i] not in self.special_tokens) or converted[i] in string.punctuation:\n",
    "                reminder=False\n",
    "                if converted[i] in string.punctuation:\n",
    "                    reminder = True\n",
    "                indices.append([i])\n",
    "            else:\n",
    "                if converted[i] not in self.special_tokens and indices!=[]:   \n",
    "                    # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "        return indices\n",
    "\n",
    "    def mask(self, text):\n",
    "        if self.use_context:\n",
    "            return self.context_aware_mask(text)\n",
    "        t = self.tokenizer(text, return_tensors='pt') # prepare normal tokenized input\n",
    "        if self.tokenizer_type == \"BPE\":\n",
    "            indices = self.get_indices_BPE(t)\n",
    "        elif self.tokenizer_type == \"WordPiece\":\n",
    "            indices = self.get_indices_WordPiece(t)\n",
    "        return indices, t, self.tokenizer.decode(t.input_ids[0]), [[]]*len(indices)   # []s for empty context\n",
    "        \n",
    "#----------------------------CONTEXT AWARE MASKING---------------------------------#\n",
    "    def find_same_tokens(self, lst, index):\n",
    "        target_value = lst[index]\n",
    "        return [i for i, value in enumerate(lst) if value == target_value and i != index]\n",
    "    \n",
    "    def get_context_indices_WordPiece(self, t):\n",
    "        converted = self.tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        words = []\n",
    "    \n",
    "        # first, getting the indices as above, but also saving the words in lowercase\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            if converted[i][:2] != self.continuation_marker and converted[i] not in self.special_tokens:\n",
    "                indices.append([i])\n",
    "                words.append(converted[i].lower())\n",
    "            else:\n",
    "                if converted[i] not in self.special_tokens and indices!=[]:   \n",
    "                    # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "                    words[-1] += converted[i][2:].lower()\n",
    "        \n",
    "        indices_context=[]\n",
    "        lem_words = [t.lemma_ for t in lem(\" \".join(words))]\n",
    "        assert len(lem_words)==len(indices), \"Issues with masking the sentence.\"\n",
    "    \n",
    "        # then, map same words to same context: eg. \"Ville\" in indices [2,3] and [13,14]\n",
    "        # => context for first is [13,14] and [2,3] for second.\n",
    "        for i in range(len(lem_words)):\n",
    "            ind_of_words = self.find_same_tokens(lem_words, i)\n",
    "            if ind_of_words != []:\n",
    "                print(words[i],\":\", ind_of_words, np.array(words)[ind_of_words])\n",
    "                current = []\n",
    "                for j in ind_of_words:\n",
    "                    current+= indices[j]\n",
    "                indices_context.append(current)\n",
    "            else:\n",
    "                indices_context.append([])\n",
    "        \n",
    "        assert len(indices)==len(indices_context), \"Issues with context masking, \"+str(len(indices))+\"!=\"+str(len(indices_context))+\"\\nIndices:\\t\"+str(indices)+\"\\nContext:\\t\"+str(indices_context)\n",
    "        return indices, indices_context \n",
    "    \n",
    "    def get_context_indices_BPE(self, t):\n",
    "        converted = self.tokenizer.convert_ids_to_tokens(t[\"input_ids\"][0])\n",
    "        indices=[]\n",
    "        words = []\n",
    "    \n",
    "        reminder = False   # for BPE, to separate punct, we need to know if the last token was punct\n",
    "        for i in range(0, len(t[\"input_ids\"][0])):\n",
    "            # for BPE, continuation marker is actually a \"starting marker\"\n",
    "            if reminder or (converted[i][0] == self.continuation_marker and converted[i] not in self.special_tokens) or converted[i] in string.punctuation:\n",
    "                reminder=False\n",
    "                if converted[i] in string.punctuation:\n",
    "                    reminder = True\n",
    "                indices.append([i])\n",
    "                if converted[i][0] == self.continuation_marker:\n",
    "                    words.append(converted[i][1:].lower())\n",
    "                else:\n",
    "                    words.append(converted[i].lower())\n",
    "            else:\n",
    "                if converted[i] not in self.special_tokens and indices!=[]:   \n",
    "                    # here we are only skipping the fact that first token is a special token; indices is empty.\n",
    "                    indices[-1].append(i)\n",
    "                    words[-1] += converted[i].lower()\n",
    "    \n",
    "        indices_context=[]\n",
    "        lem_words = [t.lemma_ for t in lem(\" \".join(words))]\n",
    "        assert len(lem_words)==len(indices), \"Issues with masking the sentence.\"\n",
    "    \n",
    "        # then, map same words to same context: eg. \"Ville\" in indices [2,3] and [13,14]\n",
    "        # => context for first is [13,14] and [2,3] for second.\n",
    "        for i in range(len(words)):\n",
    "            ind_of_words = self.find_same_tokens(lem_words, i)\n",
    "            if ind_of_words != []:\n",
    "                print(words[i],\":\", ind_of_words, np.array(words)[ind_of_words])\n",
    "                current = []\n",
    "                for j in ind_of_words:\n",
    "                    current+= indices[j]\n",
    "                indices_context.append(current)\n",
    "            else:\n",
    "                indices_context.append([])\n",
    "        \n",
    "        assert len(indices)==len(indices_context), \"Issues with context masking, \"+str(len(indices))+\"!=\"+str(len(indices_context))+\"\\nIndices:\\t\"+str(indices)+\"\\nContext:\\t\"+str(indices_context)\n",
    "        return indices, indices_context \n",
    "    \n",
    "    \n",
    "    def context_aware_mask(self, text):\n",
    "        \n",
    "        t = self.tokenizer(text, return_tensors='pt') # prepare normal tokenized input\n",
    "        if self.tokenizer_type == \"BPE\":\n",
    "            indices, context = self.get_context_indices_BPE(t)\n",
    "        elif self.tokenizer_type == \"WordPiece\":\n",
    "            indices,  context = self.get_context_indices_WordPiece(t)\n",
    "    \n",
    "        return indices, t, self.tokenizer.decode(t.input_ids[0]), context\n",
    "\n",
    "\n",
    "#-------------------------------predictions-------------------------------------#\n",
    "\n",
    "    def predict(self, masked, i, true_token, print_results=False):\n",
    "\n",
    "        def to_probability(A):\n",
    "            softmax = torch.nn.Softmax(dim=0)\n",
    "            return softmax(A)\n",
    "            \n",
    "        # do a prediction\n",
    "        with torch.no_grad():\n",
    "            model_out = self.model(**masked)\n",
    "        if self.tokenizer_type==\"WordPiece\":\n",
    "            logits = model_out[\"prediction_logits\"]\n",
    "        elif self.tokenizer_type==\"BPE\":\n",
    "            logits = model_out[\"logits\"]\n",
    "    \n",
    "        # logits for this word specifically\n",
    "        logits_i = logits[0,i,:]  # this contains the probabilities for this token\n",
    "        # change to probability\n",
    "        probs = to_probability(logits_i)\n",
    "        # true token is the index\n",
    "        word_probability = probs[true_token]\n",
    "\n",
    "        top_logits, top_tokens = torch.sort(logits, dim=2, descending=True)#[:,:,:self.choose_n]\n",
    "        top_tokens = top_tokens[:,:,:self.choose_n]\n",
    "        top_guesses = [self.tokenizer.decode(g) for g in top_tokens[0,i,:]]\n",
    "        #if print_results: print(f'{self.tokenizer.decode(true_token)} has best guesses {top_guess} and probability {word_probability}')\n",
    "\n",
    "        \n",
    "        # Do only in debug mode:\n",
    "        if print_results:\n",
    "            print(f'{self.tokenizer.decode(true_token)} has probability {word_probability}')\n",
    "            # see choose_n predictions for debug\n",
    "            top_probs = to_probability(top_logits[0,i,:])\n",
    "            top_logits = top_logits[:,:,:self.choose_n]\n",
    "            top_tokens = top_tokens[:,:,:self.choose_n]\n",
    "    \n",
    "        \n",
    "            #print(\"Guesses:\",self.tokenizer.decode(top_tokens[0,i,:]))\n",
    "            #print(\"Logits: \",top_logits[0,i,:])\n",
    "            #print(\"Probs:  \",top_probs[:self.choose_n])\n",
    "            print(\"\")\n",
    "        return word_probability, top_guesses\n",
    "\n",
    "    def get_scores(self, to_be_masked, tokens, context, debug):\n",
    "        \"\"\"\n",
    "        Calculates the (aggregated) probability of the given word based on the model prediction.\n",
    "        For multi-subtoken words, aggregation strategy is gradual unmasking and multiplication.\n",
    "        Input: \n",
    "            tokens: tokenizer output for a span of text\n",
    "            to_be_masked: indices for which are masked from the tokens and over which we calculate\n",
    "                          i.e. indices of the subtokens that form a word.\n",
    "            debug (False): prints out extra information if True\n",
    "        Returns:\n",
    "            (aggregated) probability \\in (0,1)\n",
    "        \"\"\"\n",
    "        # initialize the score; we're multiplying, so 1\n",
    "        final_score = 1\n",
    "        predictions = []\n",
    "    \n",
    "        # loop over the subtokens of a word\n",
    "        for i in range(len(to_be_masked)):\n",
    "            # making a deep copy as tensors are nested and yada yada\n",
    "            t = copy.deepcopy(tokens)\n",
    "            current = to_be_masked[i:]   # this is the token we are CURRENTLY interested in\n",
    "            for j in current:\n",
    "                t[\"input_ids\"][0][j] = self.tokenizer.mask_token_id     # we mask the SUBtokens that are in current\n",
    "            if context != []:   # if we have context, mask that\n",
    "                for j in context:\n",
    "                    t[\"input_ids\"][0][j] = self.tokenizer.mask_token_id \n",
    "            if debug:\n",
    "                print(self.tokenizer.decode(t[\"input_ids\"][0]))\n",
    "            # multiply the final score with the predicted probability => aggregates over to_be_masked==one word\n",
    "            score, preds = self.predict(t, current[0], tokens.input_ids[0][current[0]], print_results=debug)\n",
    "            final_score *= score\n",
    "            predictions.append(preds)\n",
    "            \n",
    "        return final_score, predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70e775a-6135-4bd0-8c16-0d481503c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a62497b-ddfc-413c-8ef1-cefcfc01ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForPreTraining.from_pretrained(MODEL_NAME)\n",
    "\n",
    "pf = PiiMasker(model, tokenizer, 1e-4, use_context=True, tokenizer_type=\"WordPiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1edfea1-3936-4ee7-99a6-d6c5df753a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amanda : [5, 10] ['amandalle' 'amanda']\n",
      ". : [13, 15, 18] ['.' '.' '.']\n",
      "amandalle : [3, 10] ['amanda' 'amanda']\n",
      "viestiä : [16] ['viesti']\n",
      "amanda : [3, 5] ['amanda' 'amandalle']\n",
      ". : [4, 15, 18] ['.' '.' '.']\n",
      ". : [4, 13, 18] ['.' '.' '.']\n",
      "viesti : [8] ['viestiä']\n",
      ". : [4, 13, 15] ['.' '.' '.']\n",
      "[[1], [2], [3], [4, 5], [6], [7, 8], [9], [10], [11], [12], [13, 14, 15], [16], [17], [18], [19], [20], [21], [22], [23]]\n",
      "[CLS] [MASK], olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail. com. Viesti suomeksi. [SEP]\n",
      "Moi has probability 0.07974661141633987\n",
      "\n",
      "[CLS] Moi [MASK] olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail. com. Viesti suomeksi. [SEP]\n",
      ", has probability 0.927946150302887\n",
      "\n",
      "[CLS] Moi, [MASK] Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail. com. Viesti suomeksi. [SEP]\n",
      "olen has probability 0.3216909170150757\n",
      "\n",
      "[CLS] Moi, olen [MASK] [MASK]. [MASK] [MASK] voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "Aman has probability 0.0005604944308288395\n",
      "\n",
      "[CLS] Moi, olen Aman [MASK]. [MASK] [MASK] voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "##da has probability 1.7418433344573714e-05\n",
      "\n",
      "[CLS] Moi, olen Amanda [MASK] Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail [MASK] com [MASK] Viesti suomeksi [MASK] [SEP]\n",
      ". has probability 0.7055069208145142\n",
      "\n",
      "[CLS] Moi, olen [MASK] [MASK]. [MASK] [MASK] voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "Aman has probability 5.99039094595355e-06\n",
      "\n",
      "[CLS] Moi, olen [MASK] [MASK]. Aman [MASK] voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "##dalle has probability 3.2999887480400503e-06\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle [MASK] laittaa viestiä osoitteeseen amanda @ gmail. com. Viesti suomeksi. [SEP]\n",
      "voit has probability 0.20436139404773712\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit [MASK] viestiä osoitteeseen amanda @ gmail. com. Viesti suomeksi. [SEP]\n",
      "laittaa has probability 0.8115858435630798\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa [MASK] osoitteeseen amanda @ gmail. com. [MASK] suomeksi. [SEP]\n",
      "viestiä has probability 0.10689828544855118\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa viestiä [MASK] amanda @ gmail. com. Viesti suomeksi. [SEP]\n",
      "osoitteeseen has probability 0.7364255785942078\n",
      "\n",
      "[CLS] Moi, olen [MASK] [MASK]. [MASK] [MASK] voit laittaa viestiä osoitteeseen [MASK] [MASK] [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "am has probability 0.002223519841209054\n",
      "\n",
      "[CLS] Moi, olen [MASK] [MASK]. [MASK] [MASK] voit laittaa viestiä osoitteeseen am [MASK] [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "##and has probability 3.0513435831380775e-06\n",
      "\n",
      "[CLS] Moi, olen [MASK] [MASK]. [MASK] [MASK] voit laittaa viestiä osoitteeseen amand [MASK] @ gmail. com. Viesti suomeksi. [SEP]\n",
      "##a has probability 0.00020234314433764666\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda [MASK] gmail. com. Viesti suomeksi. [SEP]\n",
      "@ has probability 0.9759264588356018\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ [MASK]. com. Viesti suomeksi. [SEP]\n",
      "gmail has probability 0.3737126588821411\n",
      "\n",
      "[CLS] Moi, olen Amanda [MASK] Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail [MASK] com [MASK] Viesti suomeksi [MASK] [SEP]\n",
      ". has probability 0.9994895458221436\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail. [MASK]. Viesti suomeksi. [SEP]\n",
      "com has probability 0.9996353387832642\n",
      "\n",
      "[CLS] Moi, olen Amanda [MASK] Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail [MASK] com [MASK] Viesti suomeksi [MASK] [SEP]\n",
      ". has probability 0.8602737188339233\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa [MASK] osoitteeseen amanda @ gmail. com. [MASK] suomeksi. [SEP]\n",
      "Viesti has probability 0.002133478643372655\n",
      "\n",
      "[CLS] Moi, olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail. com. Viesti [MASK]. [SEP]\n",
      "suomeksi has probability 0.0015139918541535735\n",
      "\n",
      "[CLS] Moi, olen Amanda [MASK] Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail [MASK] com [MASK] Viesti suomeksi [MASK] [SEP]\n",
      ". has probability 0.683881938457489\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decoded_text': '[CLS] Moi, olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda @ gmail. com. Viesti suomeksi. [SEP]',\n",
       " 'to_redact_indices': [[4, 5], [7, 8], [13, 14, 15]],\n",
       " 'to_redact_words': [['Aman', '##da'],\n",
       "  ['Aman', '##dalle'],\n",
       "  ['am', '##and', '##a']],\n",
       " 'predictions': [[['kiinnostunut',\n",
       "    'nuori',\n",
       "    '24',\n",
       "    'myös',\n",
       "    'siis',\n",
       "    '17',\n",
       "    'tyttö',\n",
       "    'itse',\n",
       "    '23',\n",
       "    '22',\n",
       "    '16',\n",
       "    '18',\n",
       "    '25',\n",
       "    'nainen',\n",
       "    '19',\n",
       "    'naispuol',\n",
       "    'ihan',\n",
       "    '20',\n",
       "    'jo',\n",
       "    '26',\n",
       "    '21',\n",
       "    'kirjoittanut',\n",
       "    'lukenut',\n",
       "    'täällä',\n",
       "    '15',\n",
       "    '14',\n",
       "    '27',\n",
       "    'juuri',\n",
       "    'vasta',\n",
       "    'suomalainen',\n",
       "    'uusi',\n",
       "    'muuttamassa',\n",
       "    'muuttanut',\n",
       "    'suomenruotsa',\n",
       "    '13',\n",
       "    'Jenni',\n",
       "    'kotoisin',\n",
       "    '28',\n",
       "    'todella',\n",
       "    'tosi',\n",
       "    'lähdössä',\n",
       "    'aika',\n",
       "    '31',\n",
       "    '.',\n",
       "    'itsekin',\n",
       "    'nyt',\n",
       "    'aivan',\n",
       "    '29',\n",
       "    'asiakaspalvelu',\n",
       "    'tamper',\n",
       "    '30',\n",
       "    'ja',\n",
       "    'ihastunut',\n",
       "    'käynyt',\n",
       "    'nais',\n",
       "    'suomen',\n",
       "    'turkula',\n",
       "    'oul',\n",
       "    'samaa',\n",
       "    'australi',\n",
       "    'vain',\n",
       "    'thaima',\n",
       "    'mies',\n",
       "    'Outi',\n",
       "    'tulossa',\n",
       "    'Jessi',\n",
       "    'menossa',\n",
       "    'kaksikiel',\n",
       "    '48',\n",
       "    'pieni',\n",
       "    '33',\n",
       "    'melko',\n",
       "    'lähettänyt',\n",
       "    '47',\n",
       "    'kakso',\n",
       "    'yksi',\n",
       "    'tamperela',\n",
       "    'poika',\n",
       "    'löytänyt',\n",
       "    'saanut',\n",
       "    'Ahvenan',\n",
       "    'ollut',\n",
       "    'varasi',\n",
       "    'perhee',\n",
       "    'Brit',\n",
       "    'Jasmin',\n",
       "    'varattu',\n",
       "    'erittäin',\n",
       "    'saksa',\n",
       "    'syntynyt',\n",
       "    'opiskellut',\n",
       "    'Hei',\n",
       "    'Moi',\n",
       "    '43',\n",
       "    'aloitteli',\n",
       "    'saman',\n",
       "    'trans',\n",
       "    'hyvin',\n",
       "    '34',\n",
       "    'Sus'],\n",
       "   ['sisko',\n",
       "    'tyttö',\n",
       "    'tytär',\n",
       "    'äiti',\n",
       "    'poika',\n",
       "    'takana',\n",
       "    'fani',\n",
       "    'kanssa',\n",
       "    'omistaja',\n",
       "    'serkku',\n",
       "    'veli',\n",
       "    'kirjoittaja',\n",
       "    'ystävä',\n",
       "    'Jenni',\n",
       "    '.',\n",
       "    'nainen',\n",
       "    'lapsi',\n",
       "    'tyttöystävä',\n",
       "    'vaimo',\n",
       "    'näköinen',\n",
       "    'mies',\n",
       "    'kaveri',\n",
       "    'täti',\n",
       "    'ikäinen',\n",
       "    'alku',\n",
       "    'isä',\n",
       "    'puolella',\n",
       "    'nimi',\n",
       "    'Elli',\n",
       "    'sisar',\n",
       "    'setä',\n",
       "    'kannalla',\n",
       "    'J',\n",
       "    'lähellä',\n",
       "    'luona',\n",
       "    'sukulainen',\n",
       "    'Hei',\n",
       "    'tarpeessa',\n",
       "    'Anna',\n",
       "    'jäsen',\n",
       "    'vanhempi',\n",
       "    'kirjoittanut',\n",
       "    'seudulta',\n",
       "    'vieressä',\n",
       "    'asemassa',\n",
       "    'kasvatti',\n",
       "    'oppilas',\n",
       "    'Mona',\n",
       "    'M',\n",
       "    'Kim',\n",
       "    'Kaisa',\n",
       "    'pentu',\n",
       "    'poikaystävä',\n",
       "    'sukua',\n",
       "    'Elina',\n",
       "    'naapuri',\n",
       "    'itse',\n",
       "    'kans',\n",
       "    '[UNK]',\n",
       "    'alla',\n",
       "    'nuorempi',\n",
       "    'Milla',\n",
       "    'Jones',\n",
       "    'lähettyvillä',\n",
       "    'Anne',\n",
       "    'Moi',\n",
       "    'kotoisin',\n",
       "    'Äiti',\n",
       "    'minä',\n",
       "    'Minna',\n",
       "    'Sisko',\n",
       "    'Heidi',\n",
       "    'edessä',\n",
       "    'morsian',\n",
       "    'aloittaja',\n",
       "    'E',\n",
       "    'opiskelija',\n",
       "    'kohdalla',\n",
       "    'Ab',\n",
       "    'Jenna',\n",
       "    'oloinen',\n",
       "    'Karoliina',\n",
       "    'A',\n",
       "    'aviomies',\n",
       "    'kasvattaja',\n",
       "    'käyttäjä',\n",
       "    'Saku',\n",
       "    'puoliso',\n",
       "    'roolissa',\n",
       "    '25',\n",
       "    'L',\n",
       "    'Helsingistä',\n",
       "    'Johanna',\n",
       "    'Li',\n",
       "    'perustaja',\n",
       "    'Linda',\n",
       "    'yo',\n",
       "    'hoitaja',\n",
       "    'Mia',\n",
       "    'kannattaja']],\n",
       "  [['Sähköposti',\n",
       "    '.',\n",
       "    'Hei',\n",
       "    'mail',\n",
       "    'Sen',\n",
       "    'Viesti',\n",
       "    'Minulle',\n",
       "    'Miel',\n",
       "    'Sin',\n",
       "    'Sinun',\n",
       "    'Eli',\n",
       "    'Min',\n",
       "    'Halu',\n",
       "    'Niin',\n",
       "    'Tä',\n",
       "    'ps',\n",
       "    'Sinä',\n",
       "    'Ulkoma',\n",
       "    'Suomeksi',\n",
       "    'Mutta',\n",
       "    'Ystävä',\n",
       "    'Mei',\n",
       "    'Ja',\n",
       "    'Lisä',\n",
       "    'Mielu',\n",
       "    'Tällä',\n",
       "    'com',\n",
       "    'Jos',\n",
       "    'Edelleen',\n",
       "    'Nim',\n",
       "    'Asia',\n",
       "    'Posti',\n",
       "    'Olet',\n",
       "    'Tämän',\n",
       "    'Sinulle',\n",
       "    'Mi',\n",
       "    'Nimen',\n",
       "    'Ps',\n",
       "    'Kysy',\n",
       "    'Suomen',\n",
       "    'Voit',\n",
       "    'Lin',\n",
       "    'Ongelmat',\n",
       "    'Helpo',\n",
       "    'Mai',\n",
       "    'Mil',\n",
       "    'Aloittaja',\n",
       "    'Joten',\n",
       "    'Teks',\n",
       "    'Kommen',\n",
       "    'Tai',\n",
       "    'Kysym',\n",
       "    'Vaihtoehto',\n",
       "    'Moi',\n",
       "    'Nais',\n",
       "    'Uude',\n",
       "    'Osoit',\n",
       "    'Tse',\n",
       "    'Tied',\n",
       "    'Yhte',\n",
       "    'Il',\n",
       "    'Google',\n",
       "    'Yksityis',\n",
       "    'Ene',\n",
       "    'Mulle',\n",
       "    'Ihan',\n",
       "    'Minun',\n",
       "    'Sitä',\n",
       "    ',',\n",
       "    'Nii',\n",
       "    'Ilman',\n",
       "    'Teksti',\n",
       "    'Nyt',\n",
       "    'Tarke',\n",
       "    'Seuraa',\n",
       "    'Kesä',\n",
       "    'Kaiken',\n",
       "    'Kiitos',\n",
       "    'Sul',\n",
       "    'Henkilö',\n",
       "    'Terve',\n",
       "    'Laita',\n",
       "    'Täällä',\n",
       "    'Nimi',\n",
       "    'Nä',\n",
       "    'Net',\n",
       "    'Muista',\n",
       "    'Mahdo',\n",
       "    'Et',\n",
       "    'Netti',\n",
       "    'SI',\n",
       "    'net',\n",
       "    'Mieh',\n",
       "    'Sivu',\n",
       "    'Päivi',\n",
       "    'Ä',\n",
       "    'MI',\n",
       "    'Tässä',\n",
       "    'Yli',\n",
       "    'Tei'],\n",
       "   ['kanssa',\n",
       "    'takia',\n",
       "    'lisäksi',\n",
       "    'sinulle',\n",
       "    ',',\n",
       "    'jälkeen',\n",
       "    'vuoksi',\n",
       "    'kautta',\n",
       "    'puolesta',\n",
       "    'tiimoilta',\n",
       "    'nimellä',\n",
       "    'tapauksessa',\n",
       "    '.',\n",
       "    'osalta',\n",
       "    'mukaan',\n",
       "    'minulle',\n",
       "    'osoitteeseen',\n",
       "    'ja',\n",
       "    'nimissä',\n",
       "    'kohdalta',\n",
       "    'suhteen',\n",
       "    'ystäville',\n",
       "    'liittyen',\n",
       "    'kans',\n",
       "    '[UNK]',\n",
       "    'sivuille',\n",
       "    'sulle',\n",
       "    'toiminnasta',\n",
       "    'omistajalle',\n",
       "    'seudulta',\n",
       "    'vanhemmille',\n",
       "    'kohdalla',\n",
       "    'kunniaksi',\n",
       "    'suuntaan',\n",
       "    'viestiin',\n",
       "    'äidille',\n",
       "    'taa',\n",
       "    'puolelta',\n",
       "    'Sinulle',\n",
       "    'nimessä',\n",
       "    'nimestä',\n",
       "    'lisäks',\n",
       "    'Suomeksi',\n",
       "    'alueelta',\n",
       "    'tapahtumista',\n",
       "    'ongelmista',\n",
       "    'blogiin',\n",
       "    'sijaan',\n",
       "    'tai',\n",
       "    'itsestäsi',\n",
       "    'nimistä',\n",
       "    'perään',\n",
       "    'asiasta',\n",
       "    'sähköpostiin',\n",
       "    'perheelle',\n",
       "    'ylläpitoon',\n",
       "    'tilalle',\n",
       "    'aikana',\n",
       "    'niin',\n",
       "    'pyynnöstä',\n",
       "    'kommenttiin',\n",
       "    'kaupungille',\n",
       "    'johdosta',\n",
       "    'mulle',\n",
       "    'voitosta',\n",
       "    'Minulle',\n",
       "    'viesteihin',\n",
       "    'viesteistä',\n",
       "    'hoidosta',\n",
       "    'palstalle',\n",
       "    'kaverille',\n",
       "    'teille',\n",
       "    'lukijoille',\n",
       "    'perheestä',\n",
       "    'alle',\n",
       "    'sähköpostilla',\n",
       "    'asioista',\n",
       "    'hallitukselle',\n",
       "    'kielellä',\n",
       "    'tytölle',\n",
       "    'sivuilta',\n",
       "    'miehelle',\n",
       "    'viestistä',\n",
       "    'kavereille',\n",
       "    'käytöstä',\n",
       "    'puolelle',\n",
       "    'vaimolle',\n",
       "    'osoitteen',\n",
       "    'kuvista',\n",
       "    'naiselle',\n",
       "    'avulla',\n",
       "    'luokse',\n",
       "    'taakse',\n",
       "    'suomeksi',\n",
       "    'Hei',\n",
       "    'ohella',\n",
       "    'itsellesi',\n",
       "    'ongelmasta',\n",
       "    'pojalle',\n",
       "    'ohjelmasta']],\n",
       "  [['an',\n",
       "    'j',\n",
       "    'w',\n",
       "    'mar',\n",
       "    'y',\n",
       "    'sun',\n",
       "    'jaa',\n",
       "    'c',\n",
       "    'n',\n",
       "    'i',\n",
       "    'mi',\n",
       "    ':',\n",
       "    'm',\n",
       "    'a',\n",
       "    'g',\n",
       "    'ni',\n",
       "    'h',\n",
       "    'e',\n",
       "    'sp',\n",
       "    's',\n",
       "    'iki',\n",
       "    'pen',\n",
       "    'suomi',\n",
       "    'finn',\n",
       "    'sin',\n",
       "    'jo',\n",
       "    'z',\n",
       "    'r',\n",
       "    'ja',\n",
       "    'ir',\n",
       "    'san',\n",
       "    'tur',\n",
       "    'ad',\n",
       "    'pe',\n",
       "    'il',\n",
       "    'ka',\n",
       "    'x',\n",
       "    'per',\n",
       "    'jan',\n",
       "    '.',\n",
       "    'u',\n",
       "    'suomen',\n",
       "    'pa',\n",
       "    'satu',\n",
       "    'my',\n",
       "    'ma',\n",
       "    'am',\n",
       "    't',\n",
       "    'se',\n",
       "    'ann',\n",
       "    'ra',\n",
       "    'min',\n",
       "    'hem',\n",
       "    'ch',\n",
       "    'q',\n",
       "    'de',\n",
       "    'l',\n",
       "    'kar',\n",
       "    'miss',\n",
       "    'ty',\n",
       "    'k',\n",
       "    'era',\n",
       "    'mys',\n",
       "    'man',\n",
       "    'ana',\n",
       "    'd',\n",
       "    'kat',\n",
       "    'ta',\n",
       "    'au',\n",
       "    'ge',\n",
       "    'rus',\n",
       "    'hen',\n",
       "    'sc',\n",
       "    'fi',\n",
       "    'maa',\n",
       "    'al',\n",
       "    'pet',\n",
       "    'mir',\n",
       "    'mat',\n",
       "    'trans',\n",
       "    'b',\n",
       "    'ang',\n",
       "    'hein',\n",
       "    'da',\n",
       "    'mo',\n",
       "    'jar',\n",
       "    'ale',\n",
       "    'ly',\n",
       "    'supp',\n",
       "    'le',\n",
       "    'ar',\n",
       "    'en',\n",
       "    'at',\n",
       "    'sh',\n",
       "    'pir',\n",
       "    'v',\n",
       "    'o',\n",
       "    'f',\n",
       "    'sip',\n",
       "    'ri'],\n",
       "   ['.',\n",
       "    '-',\n",
       "    '_',\n",
       "    '/',\n",
       "    '@',\n",
       "    ':',\n",
       "    '+',\n",
       "    \"'\",\n",
       "    'ät',\n",
       "    'am',\n",
       "    ',',\n",
       "    '&',\n",
       "    '(',\n",
       "    ')',\n",
       "    'ja',\n",
       "    '=',\n",
       "    'at',\n",
       "    'an',\n",
       "    'a',\n",
       "    'o',\n",
       "    'i',\n",
       "    'tai',\n",
       "    'and',\n",
       "    'y',\n",
       "    'is',\n",
       "    'arj',\n",
       "    'in',\n",
       "    'of',\n",
       "    '!',\n",
       "    'j',\n",
       "    'im',\n",
       "    'n',\n",
       "    'ut',\n",
       "    'r',\n",
       "    'era',\n",
       "    'ist',\n",
       "    'mar',\n",
       "    'eli',\n",
       "    'ta',\n",
       "    'w',\n",
       "    'e',\n",
       "    '?',\n",
       "    '*',\n",
       "    'ad',\n",
       "    'os',\n",
       "    'tur',\n",
       "    'ar',\n",
       "    'c',\n",
       "    'jan',\n",
       "    'per',\n",
       "    'ul',\n",
       "    'ik',\n",
       "    'ham',\n",
       "    'ann',\n",
       "    'sal',\n",
       "    'u',\n",
       "    'or',\n",
       "    'au',\n",
       "    'de',\n",
       "    'x',\n",
       "    'it',\n",
       "    ';',\n",
       "    'ys',\n",
       "    'suomi',\n",
       "    'for',\n",
       "    'sur',\n",
       "    'sun',\n",
       "    'est',\n",
       "    'as',\n",
       "    '\"',\n",
       "    'es',\n",
       "    's',\n",
       "    'ira',\n",
       "    'ang',\n",
       "    'h',\n",
       "    'ab',\n",
       "    'mir',\n",
       "    'ir',\n",
       "    'tan',\n",
       "    'hav',\n",
       "    'the',\n",
       "    'el',\n",
       "    'tin',\n",
       "    'on',\n",
       "    'mi',\n",
       "    'go',\n",
       "    'z',\n",
       "    'ste',\n",
       "    'hop',\n",
       "    'supp',\n",
       "    'to',\n",
       "    'lif',\n",
       "    'pen',\n",
       "    'nor',\n",
       "    'da',\n",
       "    'b',\n",
       "    'al',\n",
       "    'natur',\n",
       "    'er',\n",
       "    't'],\n",
       "   ['_',\n",
       "    '.',\n",
       "    '-',\n",
       "    ',',\n",
       "    '@',\n",
       "    '(',\n",
       "    'a',\n",
       "    '+',\n",
       "    '\"',\n",
       "    ':',\n",
       "    'era',\n",
       "    'm',\n",
       "    '[UNK]',\n",
       "    '/',\n",
       "    ')',\n",
       "    'info',\n",
       "    'hotmail',\n",
       "    'e',\n",
       "    'pair',\n",
       "    '2',\n",
       "    'mail',\n",
       "    'black',\n",
       "    'ät',\n",
       "    'ville',\n",
       "    's',\n",
       "    'b',\n",
       "    'o',\n",
       "    \"'\",\n",
       "    'Finland',\n",
       "    '!',\n",
       "    'out',\n",
       "    'line',\n",
       "    'p',\n",
       "    'one',\n",
       "    'sukunimi',\n",
       "    '1',\n",
       "    'com',\n",
       "    'gmail',\n",
       "    'luukku',\n",
       "    'am',\n",
       "    'free',\n",
       "    'd',\n",
       "    'j',\n",
       "    'r',\n",
       "    'tyttö',\n",
       "    'fi',\n",
       "    'on',\n",
       "    'helsinki',\n",
       "    'n',\n",
       "    'up',\n",
       "    'ym',\n",
       "    'mies',\n",
       "    'aalto',\n",
       "    '24',\n",
       "    'ab',\n",
       "    'aho',\n",
       "    'at',\n",
       "    'junior',\n",
       "    'kalle',\n",
       "    'saari',\n",
       "    'k',\n",
       "    'art',\n",
       "    'poika',\n",
       "    'ja',\n",
       "    'online',\n",
       "    'niemi',\n",
       "    'an',\n",
       "    'liv',\n",
       "    '*',\n",
       "    'i',\n",
       "    'Jansson',\n",
       "    'ira',\n",
       "    'x',\n",
       "    'of',\n",
       "    'posti',\n",
       "    'y',\n",
       "    'pro',\n",
       "    'suomi',\n",
       "    'tms',\n",
       "    'v',\n",
       "    '=',\n",
       "    'studio',\n",
       "    'c',\n",
       "    'est',\n",
       "    'w',\n",
       "    '?',\n",
       "    'is',\n",
       "    '25',\n",
       "    '3',\n",
       "    'rock',\n",
       "    'cm',\n",
       "    'city',\n",
       "    'id',\n",
       "    'live',\n",
       "    'kun',\n",
       "    'oy',\n",
       "    '8',\n",
       "    'as',\n",
       "    '4',\n",
       "    'web']]],\n",
       " 'context': [[7, 8, 13, 14, 15], [4, 5, 13, 14, 15], [4, 5, 7, 8]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.find_pii(\"Moi, olen Amanda. Amandalle voit laittaa viestiä osoitteeseen amanda@gmail.com. Viesti suomeksi.\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b23e94-6206-4f08-baf8-e198a5d48185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
