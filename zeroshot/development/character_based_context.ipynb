{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56251e03-501b-4455-bd44-0f6c6594a39e",
   "metadata": {},
   "source": [
    "## Character based context as token based fails in Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de874d4e-e459-49dd-beb1-06c36ac41826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ca': [(8, 10), (17, 19)], ' cat': [(8, 11), (17, 20)], 'cat': [(9, 11), (18, 20)]}\n"
     ]
    }
   ],
   "source": [
    "# from chatgpt:\n",
    "\n",
    "def old_find_repeated_substrings(sentence):\n",
    "    def get_substrings(s, min_len=3):\n",
    "        substrings = {}\n",
    "        for start in range(len(s)):\n",
    "            for end in range(start + min_len, len(s) + 1):\n",
    "                substr = s[start:end]\n",
    "                if substr in substrings:\n",
    "                    substrings[substr].append((start, end - 1))\n",
    "                else:\n",
    "                    substrings[substr] = [(start, end - 1)]\n",
    "        return substrings\n",
    "\n",
    "    all_substrings = get_substrings(sentence)\n",
    "    repeated_substrings = {k: v for k, v in all_substrings.items() if len(v) > 1}\n",
    "    return repeated_substrings\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"I have a cat. The catto's name is Mia\"\n",
    "result = old_find_repeated_substrings(sentence)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03005a91-a01f-40f0-bd72-eaead779f1cc",
   "metadata": {},
   "source": [
    "Does not fully work; asking for modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ce2676b-ed25-4473-8681-2d09288a48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_repeated_substrings(sentence):\n",
    "    def get_substrings(s, min_len=4):\n",
    "        substrings = {}\n",
    "        for match in re.finditer(r'\\b\\w+', s):\n",
    "            start = match.start()\n",
    "            word = match.group()\n",
    "            for i in range(len(word) - min_len + 1):\n",
    "                for j in range(i + min_len, len(word) + 1):\n",
    "                    substr = word[i:j]\n",
    "                    actual_start = start + i\n",
    "                    actual_end = start + j - 1\n",
    "                    if substr in substrings:\n",
    "                        substrings[substr].append((actual_start, actual_end))\n",
    "                    else:\n",
    "                        substrings[substr] = [(actual_start, actual_end)]\n",
    "        return substrings\n",
    "    \n",
    "    all_substrings = get_substrings(sentence)\n",
    "    \n",
    "    # Filter substrings to only those that appear more than once\n",
    "    # this bit written without chatgpt\n",
    "    repeated_substrings = {k: v for k, v in all_substrings.items() if len(v) > 1 and all([v_ not in range(v[1][0], v[1][1]) for v_ in v[0]])}\n",
    "\n",
    "    maximal_substrings = {}\n",
    "    for substr, positions in repeated_substrings.items():\n",
    "        if not any(substr in other and len(substr) < len(other) for other in repeated_substrings):\n",
    "            maximal_substrings[substr] = positions\n",
    "    \n",
    "    return maximal_substrings\n",
    "\n",
    "# Example usage:\n",
    "#sentence = \"I have a cat. The catto's name is Mia\"\n",
    "#result = find_repeated_substrings(sentence)\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24c8c5d1-47c4-4c6d-a868-5c881eb0ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amanda': [(26, 31), (34, 39)]}\n",
      "Amanda\n",
      "Amanda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Sairaanhoitajanne nimi on Amanda. Amandalle voi soittaa numeroon 0442003333.\"\n",
    "result = find_repeated_substrings(sentence)\n",
    "print(result)\n",
    "print(sentence[26:31+1])\n",
    "print(sentence[34:39+1])\n",
    "print(sentence[81:84+1])\n",
    "print(sentence[90:93+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6040395-5c5a-4466-ac19-c91e098465f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tokens = [\"Sairaanhoitajanne\", \"nimi\", \"on\", \"Ama\", \"#nda\",\"#.\", \"Aman\", \"#dalle\", \"voi\", \"soittaa\", \"nume\", \"#roon\", \"044\", \"#2003333\", \"#.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cab13cc-8166-4d61-8998-16fa1367e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(26, 31), range(34, 39)]\n",
      "-------------------\n",
      "0 Sairaanhoitajanne\n",
      "now:  0 sentence[now] S\n",
      "-------------------\n",
      "1 nimi\n",
      "now:  18 sentence[now] n\n",
      "-------------------\n",
      "2 on\n",
      "now:  23 sentence[now] o\n",
      "-------------------\n",
      "3 Ama\n",
      "now:  26 sentence[now] A\n",
      "adding for word Ama\n",
      "-------------------\n",
      "4 #nda\n",
      "now:  30 sentence[now] d\n",
      "adding for word #nda\n",
      "-------------------\n",
      "5 #.\n",
      "now:  32 sentence[now] .\n",
      "-------------------\n",
      "6 Aman\n",
      "now:  32 sentence[now] .\n",
      "-------------------\n",
      "7 #dalle\n",
      "now:  37 sentence[now] n\n",
      "adding for word #dalle\n",
      "-------------------\n",
      "8 voi\n",
      "now:  41 sentence[now] l\n",
      "-------------------\n",
      "9 soittaa\n",
      "now:  45 sentence[now] o\n",
      "-------------------\n",
      "10 nume\n",
      "now:  53 sentence[now] a\n",
      "-------------------\n",
      "11 #roon\n",
      "now:  58 sentence[now] m\n",
      "-------------------\n",
      "12 044\n",
      "now:  61 sentence[now] o\n",
      "-------------------\n",
      "13 #2003333\n",
      "now:  65 sentence[now] 0\n",
      "-------------------\n",
      "14 #.\n",
      "now:  71 sentence[now] 3\n",
      "[3, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "continuation_marker=\"#\"\n",
    "def which_to_mask(char_ids, tokens):\n",
    "    char_ranges = [range(*i) for v in char_ids.values() for i in v]\n",
    "    print(char_ranges)\n",
    "    #print(char_ranges)\n",
    "    indices = []\n",
    "    current=0\n",
    "    now=0\n",
    "    #for word, char_i in char_ids.items():\n",
    "    for i,t in enumerate(tokens):\n",
    "        print(\"-------------------\")\n",
    "        print(i,t)\n",
    "        print(\"now: \", now, \"sentence[now]\", sentence[now])\n",
    "        if any([now in v for v in char_ranges]):\n",
    "            print(f'adding for word {t}')\n",
    "            indices.append(i)\n",
    "        if t[0] == continuation_marker:\n",
    "            now += len(t)-len(continuation_marker)-1\n",
    "        else:\n",
    "            now += len(t)+1\n",
    "    return indices\n",
    "\n",
    "print(which_to_mask(find_repeated_substrings(sentence), fake_tokens))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0681635-e8b9-40ef-8e08-c883c939b2e9",
   "metadata": {},
   "source": [
    "## Okay, problems. Moving onto aligning function from other project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72fc27d3-575e-42b6-8604-cffc6c5c68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_repeated_substrings(sentence):\n",
    "    def get_substrings(s, min_len=4):\n",
    "        substrings = {}\n",
    "        for match in re.finditer(r'\\b\\w+', s):\n",
    "            start = match.start()\n",
    "            word = match.group()\n",
    "            for i in range(len(word) - min_len + 1):\n",
    "                for j in range(i + min_len, len(word) + 1):\n",
    "                    substr = word[i:j]\n",
    "                    actual_start = start + i\n",
    "                    actual_end = start + j - 1\n",
    "                    if substr in substrings:\n",
    "                        substrings[substr].append((actual_start, actual_end))\n",
    "                    else:\n",
    "                        substrings[substr] = [(actual_start, actual_end)]\n",
    "        return substrings\n",
    "    \n",
    "    all_substrings = get_substrings(sentence)\n",
    "    \n",
    "    # Filter substrings to only those that appear more than once\n",
    "    # this bit written without chatgpt; removing only one found and those that overlap (e.g. 3333 => 333 and 333)\n",
    "    repeated_substrings = {k: v for k, v in all_substrings.items() if len(v) > 1 and all([v_ not in range(v[1][0], v[1][1]) for v_ in v[0]])}\n",
    "\n",
    "    maximal_substrings = {}\n",
    "    for substr, positions in repeated_substrings.items():\n",
    "        if not any(substr in other and len(substr) < len(other) for other in repeated_substrings):\n",
    "            maximal_substrings[substr] = positions\n",
    "    \n",
    "    return maximal_substrings\n",
    "\n",
    "def make_score_mask(sentence, substrings):\n",
    "    # Tokenize the original sentence\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "    # Initialize vector with 0s\n",
    "    vector = [0] * len(tokens)\n",
    "\n",
    "    # Mark tokens that are part of maximal substrings\n",
    "    for substr, positions in substrings.items():\n",
    "        for start, end in positions:\n",
    "            substr_token_indices = [i for i, token in enumerate(tokens) if sentence.find(token, start) == start]\n",
    "            for index in substr_token_indices:\n",
    "                vector[index] = 1\n",
    "\n",
    "    return tokens, vector\n",
    "\n",
    "def align(parsed, tokenized, scores):\n",
    "    \"\"\"\n",
    "    Function to align two different tokenizations of the same text.\n",
    "    E.g. for Chinese, the model tokenizer might tokenize\n",
    "    母语 (mother tongue) as \"母\" and \"语\".\n",
    "    -> if you have defined a better tokenized, give both tokenizations\n",
    "    to this function and it will align them and the scores\n",
    "    associated with the model tokenizer.\n",
    "    All punctuation is removed for this tokenization.\n",
    "    -> would be removed in the keyword extraction step anyway.\n",
    "    \"\"\"\n",
    "\n",
    "    parsed = [re.sub(r'[^\\D\\s]|[^\\w\\s]', '', p) for p in parsed if any(j.isalpha() for j in p)]\n",
    "    to_be_dropped = [i for i in range(len(tokenized)) if any(j.isalpha() for j in tokenized[i]) and tokenized[i] not in special_tokens]\n",
    "    tokenized = [re.sub(r'[^\\D\\s]|[^\\w\\s]', '',tokenized[i]) for i in to_be_dropped]\n",
    "    scores = [scores[i] for i in to_be_dropped]\n",
    "\n",
    "\n",
    "    assert \"\".join(parsed)==\"\".join(tokenized), ASSERTION_MSG(parsed, tokenized, special_tokens)\n",
    "\n",
    "    # align\n",
    "    # t_ind contains the current index we're at now in the tokenized (by model) sentence\n",
    "    t_ind=0\n",
    "    agg_scores = np.zeros(len(parsed))\n",
    "    # for each \"real tokenization\" (here \"parsed\")\n",
    "    for p_ind in range(len(parsed)):\n",
    "        sub_scores = []\n",
    "        for p in parsed[p_ind]:     # for each char in that tokenization\n",
    "            if p == tokenized[t_ind][0]:        # if theres a match\n",
    "                sub_scores.append(scores[t_ind])    # add the score for the character\n",
    "                if len(tokenized[t_ind])>1:         # remove the character for next calculation\n",
    "                    tokenized[t_ind] = tokenized[t_ind][1:]\n",
    "                else:\n",
    "                    tokenized[t_ind]=\"-\"        # not necessary but helped debug\n",
    "                    t_ind+=1                    # move to next index\n",
    "            else:\n",
    "                raise Exception(\"Alignment impossible for unforeseen reasons.\")       # despite assertion, something went wrong\n",
    "        agg_scores[p_ind] = np.mean(sub_scores)     # aggregate the scores TODO: method\n",
    "\n",
    "    return parsed, agg_scores.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#align(parsed_text, tokenized_text, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83916173-05eb-4b8d-a048-f4fe67a358c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amanda': [(26, 31), (34, 39)], 'defg': [(82, 85), (90, 93)]}\n",
      "Sairaanhoitajanne 0\n",
      "nimi 0\n",
      "on 0\n",
      "Amanda 1\n",
      ". 0\n",
      "Amandalle 1\n",
      "voi 0\n",
      "soittaa 0\n",
      "numeroon 0\n",
      "0442003333 0\n",
      ". 0\n",
      "ab 0\n",
      "c 0\n",
      "defg 1\n",
      "abcdefg 0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Sairaanhoitajanne nimi on Amanda. Amandalle voi soittaa numeroon 0442003333. ab c defg abcdefg\"\n",
    "result = find_repeated_substrings(sentence)\n",
    "print(result)\n",
    "tok_sentence, mask = make_score_mask(sentence, result)\n",
    "for t, m in zip(tok_sentence, mask):\n",
    "    print(t,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327cb06-3f7d-4988-a5fc-02c475e0790b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
